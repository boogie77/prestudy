---
layout: post
title: 机器学习
lead: 机器学习概述
date: 2016-10-17T00:00:00.000Z
categories: 人工智能
tagline: 机器学习
tags:
  - 机器学习
  - 神经网络
  - 深度学习
  - 人工智能
---

# 机器学习（Machine Learning）

机器学习(Machine Learning， ML)是一门多领域交叉学科，涉及概率论、统计学、[逼近论](http://baike.baidu.com/view/754499.htm)、[凸分析](http://baike.baidu.com/view/7828340.htm)、[算法复杂度](http://baike.baidu.com/view/7527.htm)理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。

它是[人工智能](http://baike.baidu.com/view/2949.htm)的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。

## 机器学习研究的意义

学习是人类具有的一种重要智能行为，但究竟什么是学习，长期以来却众说纷纭。社会学家、逻辑学家和心理学家都各有其不同的看法。

比如，Langley（1996) 定义的机器学习是“机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在[经验](http://baike.baidu.com/view/21717.htm)学习中改善具体算法的性能”。（Machine learning is a science of the artificial. The field's main objects of study are artifacts， specifically algorithms that improve their performance with experience.'）

Tom Mitchell的机器学习(1997)对[信息论](http://baike.baidu.com/view/15076.htm)中的一些概念有详细的解释，其中定义机器学习时提到，“机器学习是对能通过[经验](http://baike.baidu.com/view/21717.htm)自动改进的计算机算法的研究”。（Machine Learning is the study of computer algorithms that improve automatically through experience.）

Alpaydin（2004）同时提出自己对机器学习的定义，“机器学习是用数据或以往的[经验](http://baike.baidu.com/view/21717.htm)，以此优化计算机程序的性能标准。”（Machine learning is programming computers to optimize a performance criterion using example data or past experience.）

尽管如此，为了便于进行讨论和估计学科的进展，有必要对机器学习给出定义，即使这种定义是不完全的和不充分的。顾名思义， 机器学习是研究如何使用机器来模拟人类[学习活动](http://baike.baidu.com/view/6026627.htm)的一门学科。稍为严格的提法是：机器学习是一门研究机器获取新知识和新技能，并识别现有知识的学问。这里所说的“机器”，指的就是计算机，[电子计算机](http://baike.baidu.com/view/6373.htm)，中子计算机、光子计算机或神经计算机等等。

机器能否象人类一样能具有学习能力呢？1959年[美国](http://baike.baidu.com/view/2398.htm)的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。这个程序向人们展示了机器学习的能力，提出了许多令人深思的社会问题与哲学问题。

机器的能力是否能超过人的，很多持否定意见的人的一个主要论据是：机器是人造的，其性能和动作完全是由设计者规定的，因此无论如何其能力也不会超过设计者本人。这种意见对不具备学习能力的机器来说的确是对的，可是对具备学习能力的机器就值得考虑了，因为这种机器的能力在应用中不断地提高，过一段时间之后，设计者本人也不知它的能力到了何种水平。

机器学习有下面几种定义： “机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。 “机器学习是对能通过经验自动改进的计算机算法的研究”。 “机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。” 一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P， if its performance at tasks in T， as measured by P， improves with experience E.

机器学习已经有了十分广泛的应用，例如：[数据挖掘](http://baike.baidu.com/view/7893.htm)、[计算机视觉](http://baike.baidu.com/view/155265.htm)、[自然语言处理](http://baike.baidu.com/view/18784.htm)、[生物特征识别](http://baike.baidu.com/view/1556066.htm)、[搜索引擎](http://baike.baidu.com/view/1154.htm)、[医学诊断](http://baike.baidu.com/view/10061325.htm)、检测[信用卡欺诈](http://baike.baidu.com/view/2006999.htm)、[证券市场](http://baike.baidu.com/view/165400.htm)分析、[DNA序列](http://baike.baidu.com/view/1009438.htm)测序、[语音](http://baike.baidu.com/view/338576.htm)和[手写](http://baike.baidu.com/view/308827.htm)识别、[战略游戏](http://baike.baidu.com/view/129563.htm)和[机器人](http://baike.baidu.com/view/2788.htm)运用。

## 发展简史

机器学习是人工智能研究较为年轻的分支，它的发展过程大体上可分为4个时期。

第一阶段是在20世纪50年代中叶到60年代中叶，属于热烈时期。

第二阶段是在20世纪60年代中叶至70年代中叶，被称为机器学习的冷静时期。

第三阶段是从20世纪70年代中叶至80年代中叶，称为复兴时期。

机器学习的最新阶段始于1986年。

机器学习进入新阶段的重要表现在下列诸方面：

(1) 机器学习已成为新的边缘学科并在高校形成一门课程。它综合[应用心理学](http://baike.baidu.com/view/46434.htm)、生物学和神经生理学以及数学、自动化和[计算机科学](http://baike.baidu.com/view/92404.htm)形成机器学习理论基础。

(2) 结合各种学习方法，取长补短的多种形式的集成学习[系统研究](http://baike.baidu.com/view/1446932.htm)正在兴起。特别是连接学习[符号学习](http://baike.baidu.com/view/6217848.htm)的耦合可以更好地解决连续性[信号处理](http://baike.baidu.com/view/642820.htm)中知识与技能的获取与求精问题而受到重视。

(3) 机器学习与人工智能各种基础问题的统一性观点正在形成。例如学习与问题求解结合进行、知识表达便于学习的观点产生了通用智能系统SOAR的组块学习。类比学习与问题求解结合的基于案例方法已成为[经验](http://baike.baidu.com/view/21717.htm)学习的重要方向。

(4) 各种学习方法的应用范围不断扩大，一部分已形成商品。归纳学习的知识获取工具已在诊断分类型专家系统中广泛使用。连接学习在声图文识别中占优势。分析学习已用于设计综合型专家系统。[遗传算法](http://baike.baidu.com/view/45853.htm)与强化学习在工程控制中有较好的应用前景。与[符号系统](http://baike.baidu.com/view/6552866.htm)耦合的[神经网络](http://baike.baidu.com/view/5348.htm)连接学习将在企业的智能管理与智能机器人运动规划中发挥作用。

(5) 与机器学习有关的学术活动空前活跃。国际上除每年一次的机器学习研讨会外，还有计算机学习理论会议以及遗传算法会议。

## 主要策略

学习是一项复杂的智能活动，学习过程与推理过程是紧密相连的，按照学习中使用推理的多少，机器学习所采用的策略大体上可分为4种——机械学习、通过传授学习、类比学习和通过事例学习。学习中所用的推理越多，系统的能力越强。

## 基本结构

表示学习系统的基本结构。环境向系统的学习部分提供某些信息，学习部分利用这些信息修改[知识库](http://baike.baidu.com/view/7976.htm)，以增进系统执行部分完成任务的效能，执行部分根据知识库完成任务，同时把获得的信息反馈给学习部分。在具体的应用中，环境，知识库和执行部分决定了具体的工作内容，学习部分所需要解决的问题完全由上述3部分确定。下面我们分别叙述这3部分对设计学习系统的影响。

影响学习系统设计的最重要的因素是环境向系统提供的信息。或者更具体地说是信息的质量。知识库里存放的是指导执行部分动作的一般原则，但环境向学习系统提供的信息却是各种各样的。如果信息的质量比较高，与一般原则的差别比较小，则学习部分比较容易处理。如果向学习系统提供的是[杂乱无章](http://baike.baidu.com/view/46037.htm)的指导执行具体动作的具体信息，则学习系统需要在获得足够数据之后，删除不必要的细节，进行总结推广，形成指导动作的一般原则，放入知识库，这样学习部分的任务就比较繁重，设计起来也较为困难。

因为学习系统获得的信息往往是不完全的，所以学习系统所进行的推理并不完全是可靠的，它总结出来的规则可能正确，也可能不正确。这要通过执行效果加以检验。正确的规则能使系统的效能提高，应予保留；[不正确](http://baike.baidu.com/view/8432820.htm)的规则应予修改或从数据库中删除。

知识库是影响学习系统设计的第二个因素。知识的表示有多种形式，比如特征向量、一阶逻辑语句、产生式规则、[语义网络](http://baike.baidu.com/view/157370.htm)和框架等等。这些表示方式各有其特点，在选择表示方式时要兼顾以下4个方面：

1. 表达能力强。
2. 易于推理。
3. 容易修改知识库。
4. [知识表示](http://baike.baidu.com/view/554826.htm)易于扩展。

对于知识库最后需要说明的一个问题是学习系统不能在全然没有任何知识的情况下凭空获取知识，每一个学习系统都要求具有某些知识理解环境提供的信息，分析比较，做出假设，检验并修改这些假设。因此，更确切地说，学习系统是对现有知识的扩展和改进。

执行部分是整个学习系统的核心，因为执行部分的动作就是学习部分力求改进的动作。同执行部分有关的问题有3个：复杂性、反馈和透明性。

## 分类

### 基于学习策略的分类

学习策略是指学习过程中系统所采用的推理策略。一个学习系统总是由学习和环境两部分组成。由环境（如书本或教师）提供信息，学习部分则实现信息转换，用能够理解的形式记忆下来，并从中获取有用的信息。在学习过程中，学生（学习部分）使用的推理越少，他对教师（环境）的依赖就越大，教师的负担也就越重。学习策略的分类标准就是根据学生实现信息转换所需的推理多少和难易程度来分类的，依从简单到复杂，从少到多的次序分为以下六种基本类型：

#### 1）机械学习 (Rote learning)

学习者无需任何推理或其它的知识转换，直接吸取环境所提供的信息。如[塞缪尔](http://baike.baidu.com/view/4876199.htm)的跳棋程序，[纽厄尔](http://baike.baidu.com/view/3599504.htm)和[西蒙](http://baike.baidu.com/view/189428.htm)的LT系统。这类学习系统主要考虑的是如何索引存贮的知识并加以利用。系统的学习方法是直接通过事先编好、构造好的程序来学习，[学习者](http://baike.baidu.com/view/5052957.htm)不作任何工作，或者是通过直接接收既定的事实和数据进行学习，对输入信息不作任何的推理。

#### 2）示教学习 (Learning from instruction或Learning by being told)

学生从环境（教师或其它信息源如教科书等）获取信息，把知识转换成内部可使用的表示形式，并将新的知识和原有知识有机地结合为一体。所以要求学生有一定程度的推理能力，但环境仍要做大量的工作。教师以某种形式提出和组织知识，以使学生拥有的知识可以不断地增加。这种学习方法和人类社会的学校教学方式相似，学习的任务就是建立一个系统，使它能接受教导和建议，并有效地存贮和应用学到的知识。不少专家系统在建立知识库时使用这种方法去实现知识获取。示教学习的一个典型应用例是FOO程序。

#### 3）演绎学习 (Learning by deduction)

学生所用的推理形式为演绎推理。推理从公理出发，经过逻辑变换推导出[结论](http://baike.baidu.com/view/782086.htm)。这种推理是"保真"变换和特化(specialization)的过程，使学生在推理过程中可以获取有用的知识。这种学习方法包含宏操作(macro-operation)学习、知识编辑和组块(Chunking)技术。演绎推理的逆过程是归纳推理。

#### 4）类比学习 (Learning by analogy)

利用二个不同领域（源域、目标域）中的知识相似性，可以通过类比，从源域的知识（包括相似的特征和其它性质）推导出目标域的相应知识，从而实现学习。类比学习系统可以使一个已有的[计算机应用系统](http://baike.baidu.com/view/300814.htm)转变为适应于新的领域，来完成原先没有设计的相类似的功能。

类比学习需要比上述三种学习方式更多的推理。它一般要求先从知识源（源域）中检索出可用的知识，再将其转换成新的形式，用到新的状况（目标域）中去。类比学习在人类科学技术发展史上起着重要作用，许多科学发现就是通过类比得到的。例如著名的[卢瑟福](http://baike.baidu.com/view/6147.htm)类比就是通过将原子结构（目标域）同太阳系（源域）作类比，揭示了原子结构的奥秘。

#### 5）基于解释的学习 (Explanation-based learning， EBL)

学生根据教师提供的目标概念、该概念的一个例子、领域理论及可操作准则，首先构造一个解释来说明为什该例子满足目标概念，然后将解释推广为目标概念的一个满足可操作准则的充分条件。EBL已被广泛应用于知识库求精和改善系统的性能。

著名的EBL系统有迪[乔恩](http://baike.baidu.com/view/486385.htm)（G.DeJong）的GENESIS，[米切尔](http://baike.baidu.com/view/59699.htm)（T.Mitchell）的LEXII和LEAP， 以及明顿（S.Minton）等的PRODIGY。

#### 6）归纳学习 (Learning from induction)

归纳学习是由教师或环境提供某概念的一些实例或反例，让学生通过归纳推理得出该概念的一般描述。这种学习的推理工作量远多于示教学习和演绎学习，因为环境并不提供一般性概念描述（如公理）。从某种程度上说，归纳学习的推理量也比类比学习大，因为没有一个类似的概念可以作为"源概念"加以取用。归纳学习是最基本的，发展也较为成熟的学习方法，在人工智能领域中已经得到广泛的研究和应用。

### 基于所获取知识的表示形式分类

学习系统获取的知识可能有：行为规则、物理对象的描述、问题求解策略、各种分类及其它用于任务实现的知识类型。

对于学习中获取的知识，主要有以下一些表示形式：

#### 1）代数表达式参数

学习的目标是调节一个固定函数形式的代数表达式参数或系数来达到一个理想的性能。

#### 2）决策树

用[决策树](http://baike.baidu.com/view/589872.htm)来划分物体的类属，树中每一内部节点对应一个物体属性，而每一边对应于这些属性的可选值，树的叶节点则对应于物体的每个基本分类。

#### 3）形式文法

在识别一个特定语言的学习中，通过对该语言的一系列表达式进行归纳，形成该语言的形式文法。

#### 4）产生式规则

产生式规则表示为条件—动作对，已被极为广泛地使用。学习系统中的学习行为主要是：生成、泛化、特化（Specialization）或合成产生式规则。

#### 5）形式逻辑表达式

形式逻辑表达式的基本成分是命题、谓词、变量、约束变量范围的语句，及嵌入的逻辑表达式。

#### 6）图和网络

有的系统采用图匹配和图转换方案来有效地比较和索引知识。

#### 7）框架和模式（schema）

每个框架包含一组槽，用于描述事物（概念和个体）的各个方面。

#### 8）计算机程序和其它的过程编码

获取这种形式的知识，目的在于取得一种能实现特定过程的能力，而不是为了推断该过程的内部结构。

#### 9）神经网络

这主要用在联接学习中。学习所获取的知识，最后归纳为一个神经网络。

#### 10）多种表示形式的组合

有时一个学习系统中获取的知识需要综合应用上述几种知识表示形式。

根据表示的精细程度，可将知识表示形式分为两大类：泛化程度高的粗粒度符号表示、泛化程度低的精粒度亚符号(sub-symbolic)表示。像决策树、形式文法、产生式规则、形式逻辑表达式、框架和模式等属于符号表示类；而代数表达式参数、图和网络、神经网络等则属亚符号表示类。

### 按应用领域分类

最主要的应用领域有：专家系统、认知模拟、规划和问题求解、[数据挖掘](http://baike.baidu.com/view/7893.htm)、网络信息服务、图象识别、故障诊断、自然语言理解、机器人和博弈等领域。

从机器学习的执行部分所反映的任务类型上看，大部分的应用研究领域基本上集中于以下两个范畴：分类和问题求解。

（1）分类任务要求系统依据已知的分类知识对输入的未知模式（该模式的描述）作分析，以确定输入模式的类属。相应的学习目标就是学习用于分类的准则（如分类规则）。

（2）问题求解任务要求对于给定的目标状态，寻找一个将当前状态转换为目标状态的动作序列；机器学习在这一领域的研究工作大部分集中于通过学习来获取能提高问题求解效率的知识（如搜索控制知识，启发式知识等）。   

### 机器学习综合分类

综合考虑各种学习方法出现的历史渊源、知识表示、推理策略、结果评估的相似性、研究人员交流的相对集中性以及应用领域等诸因素。将机器学习方法区分为以下六类：

#### 1）经验性归纳学习 (empirical inductive learning)

经验性归纳学习采用一些数据密集的经验方法（如版本空间法、ID3法，定律发现方法）对例子进行归纳学习。其例子和学习结果一般都采用属性、[谓词](http://baike.baidu.com/view/1026648.htm)、关系等符号表示。它相当于基于学习策略分类中的归纳学习，但扣除联接学习、遗传算法、加强学习的部分。

#### 2）分析学习（analytic learning）

分析学习方法是从一个或少数几个实例出发，运用领域知识进行分析。其主要特征为：

- 推理策略主要是演绎，而非归纳；
- 使用过去的问题求解经验（实例）指导新的问题求解，或产生能更有效地运用领域知识的搜索控制规则。

分析学习的目标是改善系统的性能，而不是新的概念描述。分析学习包括应用解释学习、演绎学习、多级结构组块以及宏操作学习等技术。

#### 3）类比学习

它相当于基于学习策略分类中的类比学习。在这一类型的学习中比较引人注目的研究是通过与过去经历的具体事例作类比来学习，称为基于范例的学习(case_based learning)，或简称范例学习。

#### 4）遗传算法（genetic algorithm）

遗传算法模拟生物繁殖的突变、交换和[达尔文](http://baike.baidu.com/view/6739.htm)的自然选择（在每一[生态环境](http://baike.baidu.com/view/30803.htm)中适者生存）。它把问题可能的解编码为一个向量，称为个体，向量的每一个元素称为基因，并利用目标函数（相应于[自然选择](http://baike.baidu.com/view/99842.htm)标准）对群体（个体的集合）中的每一个个体进行评价，根据评价值（适应度）对个体进行选择、交换、变异等遗传操作，从而得到新的群体。遗传算法适用于非常复杂和困难的环境，比如，带有大量噪声和无关数据、事物不断更新、问题目标不能明显和精确地定义，以及通过很长的执行过程才能确定当前行为的价值等。同神经网络一样，遗传算法的研究已经发展为人工智能的一个独立分支，其代表人物为霍勒德（J.H.Holland）。

#### 5）联接学习

典型的联接模型实现为[人工神经网络](http://baike.baidu.com/view/19743.htm)，其由称为神经元的一些简单计算单元以及单元间的加权联接组成。

#### 6）增强学习（reinforcement learning）

增强学习的特点是通过与环境的试探性（trial and error）交互来确定和优化动作的选择，以实现所谓的序列决策任务。在这种任务中，学习机制通过选择并执行动作，导致系统状态的变化，并有可能得到某种强化信号（立即回报），从而实现与环境的交互。强化信号就是对系统行为的一种标量化的奖惩。系统学习的目标是寻找一个合适的动作选择策略，即在任一给定的状态下选择哪种动作的方法，使产生的动作序列可获得某种最优的结果（如累计立即回报最大）。

在综合分类中，经验归纳学习、遗传算法、联接学习和增强学习均属于归纳学习，其中经验归纳学习采用符号表示方式，而遗传算法、联接学习和加强学习则采用亚符号表示方式；分析学习属于演绎学习。

实际上，类比策略可看成是归纳和演绎策略的综合。因而最基本的学习策略只有归纳和演绎。

从学习内容的角度看，采用归纳策略的学习由于是对输入进行归纳，所学习的知识显然超过原有系统知识库所能蕴涵的范围，所学结果改变了系统的知识演绎闭包， 因而这种类型的学习又可称为知识级学习;而采用演绎策略的学习尽管所学的知识能提高系统的效率，但仍能被原有系统的知识库所蕴涵，即所学的知识未能改变系统的演绎闭包，因而这种类型的学习又被称为符号级学习。   

### 学习形式分类

#### 1）监督学习(supervised learning)

监督学习，即在机械学习过程中提供对错指示。一般实在是数据组中包含最终结果（0，1）。通过算法让机器自我减少误差。这一类学习主要应用于分类和预测 (regression & classify)。[监督学习](http://baike.baidu.com/view/2759226.htm)从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括[回归分析](http://baike.baidu.com/view/145440.htm)和统计分类。

#### 2）非监督学习(unsupervised learning)

非监督学习又称归纳性学习（clustering）利用K方式(Kmeans)，建立中心（centriole），通过循环和递减运算(iteration&descent)来减小误差，达到分类的目的。

## 研究领域

机器学习领域的研究工作主要围绕以下三个方面进行：

### （1）面向任务的研究

研究和分析改进一组预定任务的执行性能的学习系统。

### （2）认知模型

研究人类学习过程并进行计算机模拟。

### （3）理论分析

从理论上探索各种可能的学习方法和独立于应用领域的算法

机器学习是继专家系统之后人工智能应用的又一重要研究领域，也是人工智能和神经计算的核心研究课题之一。现有的[计算机系统](http://baike.baidu.com/view/1130583.htm)和人工智能系统没有什么学习能力，至多也只有非常有限的学习能力，因而不能满足科技和生产提出的新要求。对机器学习的讨论和机器学习研究的进展，必将促使人工智能和整个科学技术的进一步发展 。

# 机器学习

机器学习研究的是计算机怎样**模拟人类的学习行为**，以获取新的知识或技能，并重新组织已有的知识结构使之不断改善自身。简单一点说，就是计算机从数据中学习出规律和模式，以应用在新数据上做预测的任务。近年来互联网数据大爆炸，数据的丰富度和覆盖面远远超出人工可以观察和总结的范畴，而机器学习的算法能指引计算机在海量数据中，挖掘出有用的价值，也使得无数学习者为之着迷。

# 机器学习关注问题

并非所有的问题都适合用机器学习解决(很多逻辑清晰的问题用规则能很高效和准确地处理)，也没有一个机器学习算法可以通用于所有问题。咱们先来了解了解，机器学习，到底关心和解决什么样的问题。

**从功能的角度分类**，机器学习在一定量级的数据上，可以解决下列问题：

> ### 分类问题
>
> 根据数据样本上抽取出的特征，判定其属于有限个类别中的哪一个。比如：
>
> - 垃圾邮件识别(结果类别：1、垃圾邮件 2、正常邮件)
> - 文本情感褒贬分析(结果类别：1、褒 2、贬)
> - 图像内容识别识别(结果类别：1、喵星人 2、汪星人 3、人类 4、草泥马 5、都不是)。
>
> ### 回归问题
>
> 根据数据样本上抽取出的特征，预测一个连续值的结果。比如：
>
> - 星爷《美人鱼》票房
> - 大帝都2个月后的房价
> - 隔壁熊孩子一天来你家几次，宠幸你多少玩具
>
> ### 聚类问题
>
> 根据数据样本上抽取出的特征，让样本抱抱团(相近/相关的样本在一团内)。比如：
>
> - google的新闻分类
> - 用户群体划分
>
> 我们再把上述常见问题划到机器学习最典型的2个分类上。
>
> - 分类与回归问题需要用已知结果的数据做训练，属于“**监督学习**”
> - 聚类的问题不需要已知标签，属于“**非监督学习**”。

如果在IT行业(尤其是互联网)里溜达一圈，你会发现机器学习在以下**热点问题**中有广泛应用：

> ### 计算机视觉
>
> 典型的应用包括：人脸识别、车牌识别、扫描文字识别、图片内容识别、图片搜索等等。
>
> ### 自然语言处理
>
> 典型的应用包括：搜索引擎智能匹配、文本内容理解、文本情绪判断，语音识别、输入法、机器翻译等等。
>
> ### 社会网络分析
>
> 典型的应用包括：用户画像、网络关联分析、欺诈作弊发现、热点发现等等。
>
> ### 推荐
>
> 典型的应用包括：虾米音乐的“歌曲推荐”，某宝的“猜你喜欢”等等。

# 入门方法与学习路径

看似学习难度大，曲线陡的机器学习，对大多数入门者也有一个比较通用的学习路径，也有一些优秀的入门资料可以降低大家的学习门槛，同时激发我们的学习乐趣。

简单说来，大概的一个学习路径如下：

简单说一点，之所以最左边写了『数学基础』『典型机器学习算法』『编程基础』三个并行的部分，是因为机器学习是一个将数学/算法理论和工程实践紧密结合的领域，需要扎实的理论基础帮助引导数据分析与模型调优，同时也需要精湛的工程开发能力去高效化地训练和部署模型和服务。

需要多说一句的是，在互联网领域从事机器学习的人，有2类背景的人比较多，其中一部分(很大一部分)是程序员出身，这类同学工程经验相对会多一些，另一部分是学数学统计领域的同学，这部分同学理论基础相对扎实一些。因此对比上图，2类同学入门机器学习，所欠缺和需要加强的部分是不一样的。

## 数学基础

有无数激情满满大步向前，誓要在机器学习领域有一番作为的同学，在看到公式的一刻突然就觉得自己狗带了。是啊，机器学习之所以相对于其他开发工作，更有门槛的根本原因就是数学。每一个算法，要在训练集上最大程度拟合同时又保证泛化能力，需要不断分析结果和数据，调优参数，这需要我们对数据分布和模型底层的数学原理有一定的理解。所幸的是如果只是想合理应用机器学习，而不是做相关方向高精尖的research，需要的数学知识啃一啃还是基本能理解下来的。至于更高深的部分，恩，博主非常愿意承认自己是『数学渣』。

基本所有常见机器学习算法需要的数学基础，都集中在微积分、线性代数和概率与统计当中。下面我们先过一过知识重点，文章的后部分会介绍一些帮助学习和巩固这些知识的资料。 

### 微积分

- **微分的计算及其几何、物理含义**，是机器学习中大多数算法的求解过程的核心。比如算法中运用到**梯度下降法、牛顿法**等。如果对其几何意义有充分的理解，就能理解“梯度下降是用平面来逼近局部，牛顿法是用曲面逼近局部”，能够更好地理解运用这样的方法。
- **凸优化和条件最优化** 的相关知识在算法中的应用随处可见，如果能有系统的学习将使得你对算法的认识达到一个新高度。

### 线性代数

- 大多数机器学习的算法要应用起来，依赖于高效的计算，这种场景下，程序员GG们习惯的多层for循环通常就行不通了，而大多数的循环操作可转化成**矩阵之间的乘法运算**，这就和线性代数有莫大的关系了
- **向量的内积运算**更是随处可见。
- 矩阵乘法与分解在机器学习的**主成分分析**（PCA）和**奇异值分解**（SVD） 等部分呈现刷屏状地出现。

### 概率与统计

从广义来说，机器学习在做的很多事情，和统计层面数据分析和发掘隐藏的模式，是非常类似的。

- **极大似然思想**、**贝叶斯模型** 是理论基础，**朴素贝叶斯**(Na?ve Bayes )、**语言模型**(N-gram)、**隐马尔科夫**（HMM）、**隐变量混合概率模型**是他们的高级形态。
- 常见分布如**高斯分布**是混合高斯模型(GMM)等的基础。

## 典型算法

绝大多数问题用典型机器学习的算法都能解决，粗略地列举一下这些方法如下：

- 处理分类问题的常用算法包括：**逻辑回归**(工业界最常用)，**支持向量机**，**随机森林**，**朴素贝叶斯**(NLP中常用)，**深度神经网络**(视频、图片、语音等多媒体数据中使用)。
- 处理回归问题的常用算法包括：**线性回归**，**普通最小二乘回归**（Ordinary Least Squares Regression），**逐步回归**（Stepwise Regression），**多元自适应回归样条**（Multivariate Adaptive Regression Splines）
- 处理聚类问题的常用算法包括：**K均值**（K-means），**基于密度聚类**，**LDA**等等。
- 降维的常用算法包括：**主成分分析**（PCA）,**奇异值分解**（SVD） 等。
- 推荐系统的常用算法：**协同过滤算法**
- 模型融合(model ensemble)和提升(boosting)的算法包括：**bagging，adaboost，GBDT，GBRT**
- 其他很重要的算法包括：**EM算法**等等。

我们多插一句，机器学习里所说的“算法”与程序员所说的“数据结构与算法分析”里的“算法”略有区别。前者更关注结果数据的**召回率、精确度、准确性**等方面，后者更关注执行过程的时间复杂度、空间复杂度等方面。 。当然，实际机器学习问题中，对效率和资源占用的考量是不可或缺的。

### 编程语言、工具和环境

看了无数的理论与知识，总归要落到实际动手实现和解决问题上。而没有工具所有的材料和框架、逻辑、思路都给你，也寸步难行。因此我们还是得需要合适的编程语言、工具和环境帮助自己在数据集上应用机器学习算法，或者实现自己的想法。对初学者而言，Python和R语言是很好的入门语言，很容易上手，同时又活跃的社区支持，丰富的工具包帮助我们完成想法。相对而言，似乎计算机相关的同学用Python多一些，而数学统计出身的同学更喜欢R一些。我们对编程语言、工具和环境稍加介绍：

### Python

python有着**全品类的数据科学工具**，从数据获取、数据清洗到整合各种算法都做得非常全面。

- 网页爬虫：

> 1. scrapy

- 数据挖掘：

> 1. pandas：模拟R，进行数据浏览与预处理。
> 2. numpy：数组运算。
> 3. scipy：高效的科学计算。
> 4. matplotlib：非常方便的数据可视化工具。

- 机器学习: 

> 1. scikit-learn：远近闻名的机器学习package。未必是最高效的，但是接口真心封装得好，几乎所有的机器学习算法输入输出部分格式都一致。而它的支持文档甚至可以直接当做教程来学习，非常用心。对于不是非常高纬度、高量级的数据，scikit-learn胜任得非常好(有兴趣可以看看sklearn的源码，也很有意思)。
> 2. libsvm：高效率的svm模型实现(了解一下很有好处，libsvm的系数数据输入格式，在各处都非常常见)
> 3. keras/TensorFlow：对深度学习感兴趣的同学，也能很方便地搭建自己的神经网络了。

- 自然语言处理: 

> 1. nltk：自然语言处理的相关功能做得非常全面，有典型语料库，而且上手也非常容  易。

- 交互式环境： 

> 1. ipython notebook：能直接打通数据到结果的通道，方便至极。强力推荐。

### R

R最大的优势是开源社区，聚集了非常多功能强大可直接使用的包，绝大多数的机器学习算法在R中都有完善的包可直接使用，同时文档也非常齐全。常见的package包括：**RGtk2, pmml, colorspace, ada, amap, arules, biclust, cba, descr, doBy, e1071, ellipse**等等。另外，值得一提的是R的可视化效果做得非常不错，而这对于机器学习是非常有帮助的。

### 其他语言

相应资深程序员GG的要求，再补充一下java和C++相关机器学习package。

- Java系列
- WEKA Machine Learning Workbench 相当于java中的scikit-learn
- 其他的工具如Massive Online Analysis（MOA）、MEKA 、 Mallet 等也非常有名。
- 更多详细的应用请参考这篇文章《25个Java机器学习工具&库》
- C++系列
- mlpack，高效同时可扩充性非常好的机器学习库。
- Shark：文档齐全的老牌C++机器学习库。

### 大数据相关

- **Hadoop**：基本上是工业界的标配了。一般用来做特征清洗、特征处理的相关工作。
- **Spark**：提供了MLlib这样的大数据机器学习平台，实现了很多常用算法。但可靠性、稳定性上有待提高。

#### 操作系统

- **mac**和**linux**会方便一些，而windows在开发中略显力不从心。所谓方便，主要是指的mac和linux在下载安装软件、配置环境更快捷。
- 对于只习惯windows的同学，推荐**anaconda**，一步到位安装完python的全品类数据科学工具包。

## 基本工作流程

以上我们基本具备了机器学习的必要条件，剩下的就是怎么运用它们去做一个完整的机器学习项目。其工作流程如下：

### 抽象成数学问题

- 明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。
- 这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个**分类还是回归或者是聚类的问题**，如果都不是的话，如果划归为其中的某类问题。

### 获取数据

- 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
- **数据要有代表性**，否则必然会过拟合。
- 而且对于分类问题，**数据偏斜不能过于严重**，不同类别的数据数量不要有数个数量级的差距。
- 而且还要**对数据的量级有一个评估**，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。

### 特征预处理与特征选择

- 良好的数据要能够提取出良好的**特征**才能真正发挥效力。
- **特征预处理、数据清洗**是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。**归一化、离散化、因子化、缺失值处理、去除共线性**等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。
- **筛选出显著特征、摒弃非显著特征**，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如**相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重**等方法。

### 训练模型与调优

- 直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是**调整这些算法的（超）参数，使得结果变得更加优良**。这需要我们==对算法的原理有深入的理解==。理解越深入，就越能发现问题的症结，提出良好的调优方案。

### 模型诊断

如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。

- **过拟合、欠拟合** 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
- **误差分析** 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……
- **诊断后的模型需要进行调优**，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。

### 模型融合

- 一般来说，**模型融合**后都能使得效果有一定提升。而且效果很好。
- 工程上，主要提升算法准确度的方法是分别在模型的前端（**特征清洗和预处理**，不同的采样模式）与后端（**模型融合**）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。

### 上线运行

- 这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其**运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性**是否可接受。

这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。

## 关于积累项目经验

初学机器学习可能有一个误区，就是一上来就陷入到对各种高大上算法的追逐当中。动不动就我能不能用深度学习去解决这个问题啊？我是不是要用boosting算法做一些模型融合啊？我一直持有一个观点，==『脱离业务和数据的算法讨论是毫无意义的』==。

实际上按我们的学习经验，从一个数据源开始，即使是用最传统，已经应用多年的机器学习算法，先完整地走完机器学习的整个工作流程，不断尝试各种算法深挖这些数据的价值，在运用过程中**把数据、特征和算法搞透**，真正积累出项目经验 才是最快、最靠谱的学习路径。

那如何获取数据和项目呢？一个捷径就是积极参加国内外各种数据挖掘竞赛，数据直接下载下来，按照竞赛的要求去不断优化，积累经验。国外的Kaggle和国内的DataCastle 以及阿里天池比赛都是很好的平台，你可以在上面获取真实的数据和数据科学家们一起学习和进行竞赛，尝试使用已经学过的所有知识来完成这个比赛本身也是一件很有乐趣的事情。和其他数据科学家的讨论能开阔视野，对机器学习算法有更深层次的认识。

有意思的是，有些平台，比如阿里天池比赛，甚至给出了从数据处理到模型训练到模型评估、可视化到模型融合增强的全部组件，你要做的事情只是参与比赛，获取数据，然后使用这些组件去实现自己的**idea**即可。具体内容可以参见阿里云机器学习文档。

## 自主学习能力

多几句嘴，这部分内容和机器学习本身没有关系，但是我们觉得这方面的能力对于任何一种新知识和技能的学习来说都是至关重要的。 自主学习能力提升后，意味着你能够跟据自己的情况，找到最合适的学习资料和最快学习成长路径。

### 信息检索过滤与整合能力

对于初学者，绝大部分需要的知识通过网络就可以找到了。

google搜索引擎技巧——组合替换搜索关键词、站内搜索、学术文献搜索、PDF搜索等——都是必备的。

一个比较好的习惯是找到信息的原始出处，如个人站、公众号、博客、专业网站、书籍等等。这样就能够找到系统化、不失真的高质量信息。

百度搜到的技术类信息不够好，建议只作为补充搜索来用。各种搜索引擎都可以交叉着使用效果更好。

学会去常见的高质量信息源中搜索东西:stackoverflow（程序相关）、quora（高质量回答）、wikipedia（系统化知识，比某某百科不知道好太多）、知乎（中文、有料）、网盘搜索（免费资源一大把）等。

将搜集到的网页放到分类齐全的云端收藏夹里，并经常整理。这样无论在公司还是在家里，在电脑前还是在手机上，都能够找到自己喜欢的东西。

搜集到的文件、代码、电子书等等也放到云端网盘里，并经常整理。

### 提炼与总结能力

经常作笔记，并总结自己学到的知识是成长的不二法门。其实主要的困难是懒，但是坚持之后总能发现知识的共性，就能少记一些东西，掌握得更多。

笔记建议放到云端笔记里，印象笔记、为知笔记都还不错。这样在坐地铁、排队等零碎的时间都能看到笔记并继续思考。

### 提问与求助能力

机器学习的相关QQ群、论坛、社区一大堆。总有人知道你问题的答案。

但是大多数同学都很忙，没法像家庭教师那样手把手告诉你怎么做。

为了让回答者最快明白你的问题，最好该学会正确的问问题的方式:陈述清楚你的业务场景和业务需求是什么，有什么已知条件，在哪个具体的节点上遇到困难了，并做过哪些努力。

有一篇经典的文章告诉你怎样通过提问获得帮助：《提问的智慧》，强力推荐。 话锋犀利了些，但里面的干货还是很好的。

别人帮助你的可能性与你提问题的具体程度和重要性呈指数相关。

### 分享的习惯

我们深信：“**证明自己真的透彻理解一个知识，最好的方法，是给一个想了解这个内容的人，讲清楚这个内容。**” 分享能够最充分地提升自己的学习水平。这也是我们坚持长期分享最重要的原因。

分享还有一个副产品，就是自己在求助的时候能够获得更多的帮助机会，这也非常重要。

# 机器学习算法分类

## 学习方式

根据数据类型的不同，对一个问题的建模有不同的方式。在机器学习或者人工智能领域，人们首先会考虑算法的学习方式。在机器学习领域，有几种主要的学习方式。将算法按照学习方式分类是一个不错的想法，这样可以让人们在建模和算法选择的时候考虑能根据输入数据来选择最合适的算法来获得最好的结果。

### 监督式学习：

在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，**监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率**。监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）

### 非监督式学习：

**在非监督式学习中，数据并不被特别标识**，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。

### 半监督式学习：

**在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测**。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。

### 强化学习：

**在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整**。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）

在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。 而强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。

## 算法类似性

根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。而对于有些分类来说，同一分类的算法可以针对不同类型的问题。这里，我们尽量把常用的算法按照最容易理解的方式进行分类。

### 回归算法：

回归算法是==试图采用对误差的衡量来探索变量之间的关系==的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：**最小二乘法**（Ordinary Least Square），**逻辑回归**（Logistic Regression），**逐步式回归**（Stepwise Regression），**多元自适应回归样条**（Multivariate Adaptive Regression Splines）以及**本地散点平滑估计**（Locally Estimated Scatterplot Smoothing）

### 基于实例的算法

基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“==赢家通吃==”学习或者“==基于记忆的学习==”。常见的算法包括 **k-Nearest Neighbor**(KNN), **学习矢量量化**（Learning Vector Quantization， LVQ），以及**自组织映射算法**（Self-Organizing Map ， SOM）

### 正则化方法

正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常==对简单模型予以奖励而对复杂算法予以惩罚==。常见的算法包括：**Ridge Regression**， **Least Absolute Shrinkage and Selection Operator**（LASSO），以及**弹性网络**（Elastic Net）。

### 决策树学习

决策树算法根据数据的属性==采用树状结构建立决策模型==， 决策树模型常常用来解决分类和回归问题。常见的算法包括：**分类及回归树**（Classification And Regression Tree， CART）， **ID3** (Iterative Dichotomiser 3)， **C4.5**， **Chi-squared Automatic Interaction Detection**(CHAID), **Decision Stump**, **随机森林**（Random Forest）， **多元自适应回归样条**（MARS）以及**梯度推进机**（Gradient Boosting Machine， GBM）

### 贝叶斯方法

贝叶斯方法算法是基于==贝叶斯定理==的一类算法，主要用来解决分类和回归问题。常见算法包括：**朴素贝叶斯算法**，**平均单依赖估计**（Averaged One-Dependence Estimators， AODE），以及**Bayesian Belief Network**（BBN）。

### 基于核的算法

基于核的算法中最著名的莫过于支持向量机（SVM）了。 基于核的算法==把输入数据映射到一个高阶的向量空间==， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。 常见的基于核的算法包括：**支持向量机**（Support Vector Machine， SVM）， **径向基函数**（Radial Basis Function ，RBF)， 以及**线性判别分析**（Linear Discriminate Analysis ，LDA)等

### 聚类算法

聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常==按照中心点或者分层的方式对输入数据进行归并==。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 **k-Means算法**以及**期望最大化算法**（Expectation Maximization， EM）。

### 关联规则学习

关联规则学习通过**寻找最能够解释数据变量之间关系的规则**，来找出大量多元数据集中有用的关联规则。常见算法包括 **Apriori算法**和**Eclat算法**等。

### 人工神经网络

==人工神经网络算法模拟生物神经网络，是一类模式匹配算法==。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：**感知器神经网络**（Perceptron Neural Network）, **反向传递**（Back Propagation）， **Hopfield网络**，**自组织映射**（Self-Organizing Map, SOM）。**学习矢量量化**（Learning Vector Quantization， LVQ）

### 深度学习

==深度学习算法是对人工神经网络的发展==。 在近期赢得了很多关注， 特别是百度也开始发力深度学习后， 更是在国内引起了很多关注。 在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：**受限波尔兹曼机**（Restricted Boltzmann Machine， RBN）， **Deep Belief Networks**（DBN），**卷积网络**（Convolutional Network）, **堆栈式自动编码器**（Stacked Auto-encoders）。

### 降低维度算法

像聚类算法一样，降低维度算法==试图分析数据的内在结构==，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：**主成份分析**（Principle Component Analysis， PCA），**偏最小二乘回归**（Partial Least Square Regression，PLS）， **Sammon映射**，**多维尺度**（Multi-Dimensional Scaling, MDS）, **投影追踪**（Projection Pursuit）等。

### 集成算法：

集成算法==用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测==。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：**Boosting**， **Bootstrapped Aggregation**（Bagging）， **AdaBoost**，**堆叠泛化**（Stacked Generalization， Blending），**梯度推进机**（Gradient Boosting Machine, GBM），**随机森林**（Random Forest）。
























