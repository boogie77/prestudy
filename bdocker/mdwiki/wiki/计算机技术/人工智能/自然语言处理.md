# 自然语言处理

用nltk 处理中文是完全可用的。其重点在于中文分词和文本表达的形式。

中文和英文主要的不同之处是中文需要分词。

因为nltk 的处理粒度一般是词，所以必须要先对文本进行分词然后再用nltk 来处理（不需要用nltk 来做分词，直接用分词包就可以了。严重推荐结巴分词，非常好用）。

中文分词之后，文本就是一个由每个词组成的长数组：[word1, word2, word3…… wordn]。之后就可以使用nltk 里面的各种方法来处理这个文本了。比如用FreqDist 统计文本词频，用bigrams 把文本变成双词组的形式：[(word1, word2), (word2, word3), (word3, word4)……(wordn-1, wordn)]。

再之后就可以用这些来计算文本词语的信息熵、互信息等。再之后可以用这些来选择机器学习的特征，构建分类器，对文本进行分类（商品评论是由多个独立评论组成的多维数组，网上有很多情感分类的实现例子用的就是nltk 中的商品评论语料库，不过是英文的。但整个思想是可以一致的）。



## 参考资料

- [如何用 Python 中的 NLTK 对中文进行分析和处理？](https://www.zhihu.com/question/20922994)
- [Natural Language Toolkit](http://www.nltk.org/)


- [NLTK book is updated for Python 3 and NLTK 3](http://www.nltk.org/book/)

- [结巴中文分词 ](https://github.com/fxsjy/jieba)

- [THULAC：一个高效的中文词法分析工具包](http://thulac.thunlp.org/)

- [python的nltk中文使用和学习资料汇总帮你入门提高 ](http://blog.csdn.net/huyoo/article/details/12188573)

- [Ansj中文分词](http://www.ansj.org/resource/20) 

- [ NLP-lang java自然语言处理基础包](http://www.ansj.org/resource/17) 


## 结巴分词

结巴的标语是：做最好的 Python 中文分词组件，或许从现在来看它没做到最好，但是已经做到了使用的人最多。结巴分词网上的学习资料和使用案例比较多，上手相对比较轻松，速度也比较快。

结巴的优点： 

- 支持三种分词模式
- 支持繁体分词
- 支持自定义词典
- MIT 授权协议



## [THULAC：一个高效的中文词法分析工具包](http://thulac.thunlp.org/)

THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：

1. 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。
2. 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。
3. 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。