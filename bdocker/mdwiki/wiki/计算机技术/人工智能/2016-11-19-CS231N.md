---
layout: post
title: CS231N
lead: Convolutional Neural Networks for Visual Recognition
date: 2016-11-19T00:00:00.000Z
categories: 人工智能
tagline: 计算机视觉
tags:
  - 计算机视觉
  - 深度学习
  - 人工智能
---

[TOC]

# CS231n简介

CS231n的全称是[CS231n: Convolutional Neural Networks for Visual Recognition](https://vision.stanford.edu/teaching/cs231n/index.html)，即**面向视觉识别的卷积神经网络**。该课程是[斯坦福大学计算机视觉实验室](https://vision.stanford.edu/index.html)推出的课程。需要注意的是，目前大家说CS231n，大都指的是2016年冬季学期（一月到三月）的最新版本。

**课程描述**：请允许我们引用课程主页上的**官方描述**如下。

> 计算机视觉在社会中已经逐渐普及，并广泛运用于搜索检索、图像理解、手机应用、地图导航、医疗制药、无人机和无人驾驶汽车等领域。而这些应用的核心技术就是图像分类、图像定位和图像探测等视觉识别任务。近期神经网络（也就是“深度学习”）方法上的进展极大地提升了这些代表当前发展水平的视觉识别系统的性能。
> 本课程将深入讲解深度学习框架的细节问题，聚焦面向视觉识别任务（尤其是图像分类任务）的端到端学习模型。在10周的课程中，学生们将会学习如何实现、训练和调试他们自己的神经网络，并建立起对计算机视觉领域的前沿研究方向的细节理解。最终的作业将包括训练一个有几百万参数的卷积神经网络，并将其应用到最大的图像分类数据库（ImageNet）上。我们将会聚焦于教授如何确定图像识别问题，学习算法（比如反向传播算法），对网络的训练和精细调整（fine-tuning）中的工程实践技巧，指导学生动手完成课程作业和最终的课程项目。本课程的大部分背景知识和素材都来源于[ImageNet Challenge](https://image-net.org/challenges/LSVRC/2014/index)竞赛。

**课程内容**：官方课程安排及资源获取请点击[这里](https://vision.stanford.edu/teaching/cs231n/syllabus.html)，课程视频请在Youtube上查看[Andrej Karpathy](https://www.youtube.com/channel/UCPk8m_r6fkUSYmvgCBwq-sw)创建的[播放列表](https://www.youtube.com/playlist%3Flist%3DPLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)。通过查看官方课程表，我们可以看到：CS231n课程资源主要由**授课视频与PPT**，**授课知识详解笔记**和**课程作业**三部分组成。其中：

- **授课视频15课**。每节课时约1小时左右，每节课一份PPT。
- **授课知识详解笔记共9份**。光看课程视频是不够的，深入理解课程笔记才能比较扎实地学习到知识。
- **课程作业3次**。其中每次作业中又包含多个小作业，完成作业能确保对于课程关键知识的深入理解和实现。
- **课程项目1个**。这个更多是面向斯坦福的学生，组队实现课程项目。
- **拓展阅读若干**。课程推荐的拓展阅读大多是领域内的经典著作节选或论文，推荐想要深入学习的同学阅读。

**课程评价**：我们觉得赞！很多人都觉得赞！当然也有人觉得不好。具体如何，大家搜搜CS231n在网络，在知乎上的评价不就好了嘛！**个人认为**：入门深度学习的**一门良心课**。**适合绝大多数**想要学习深度学习知识的人。

**课程不足**：课程后期从RCNN开始就没有课程笔记。

# 课程学习方法

三句话总结：

- **看授课视频形成概念，发现个人感兴趣方向。**
- **读课程笔记理解细节，夯实工程实现的基础。**
- **码课程作业实现算法，积累实验技巧与经验。**

引用一下学习金字塔的图，意思大家都懂的：

![哈佛案例教学法精髓](https://pic4.zhimg.com/77465c8318e6c4d40df274b92602d83f_b.png)

# CS231n全部9篇课程知识详解笔记的翻译

---

## 原文：[[python/numpy tutorial\]](https://cs231n.github.io/python-numpy-tutorial)。

翻译：[Python Numpy教程](https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit)。

> 我们将使用Python编程语言来完成本课程的所有作业。Python是一门伟大的通用编程语言，在一些常用库（numpy, scipy, matplotlib）的帮助下，它又会变成一个强大的科学计算环境。我们期望你们中大多数人对于Python语言和Numpy库比较熟悉，而对于没有Python经验的同学，这篇教程可以帮助你们快速了解Python编程环境和如何使用Python作为科学计算工具。

## Python Numpy教程

**译者注**：本文[智能单元](https://zhuanlan.zhihu.com/intelligentunit)首发，翻译自斯坦福CS231n课程笔记[Python Numpy Tutorial](https://cs231n.github.io/python-numpy-tutorial/)，由课程教师[Andrej Karpathy](https://cs.stanford.edu/people/karpathy/)授权进行翻译。本篇教程由[杜客](https://www.zhihu.com/people/du-ke)翻译完成，[Flood Sung](https://www.zhihu.com/people/flood-sung)、[SunisDown](https://www.zhihu.com/people/sunisdown)、[巩子嘉](https://www.zhihu.com/people/gong-zi-jia-57)和一位不愿透露ID的知友对本翻译亦有贡献。

这篇教程由[Justin Johnson](https://cs.stanford.edu/people/jcjohns/)创作。

我们将使用Python编程语言来完成本课程的所有作业。Python是一门伟大的通用编程语言，在一些常用库（numpy, scipy, matplotlib）的帮助下，它又会变成一个强大的科学计算环境。

我们期望你们中大多数人对于Python语言和Numpy库比较熟悉，而对于没有Python经验的同学，这篇教程可以帮助你们快速了解Python编程环境和如何使用Python作为科学计算工具。

一部分同学对于Matlab有一定经验。对于这部分同学，我们推荐阅读 [numpy for Matlab users](https://wiki.scipy.org/NumPy_for_Matlab_Users)页面。

你们还可以查看[本教程的IPython notebook版](https://github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb)。该教程是由[Volodymyr Kuleshov](https://web.stanford.edu/%257Ekuleshov/)和[Isaac Caswell](https://symsys.stanford.edu/viewing/symsysaffiliate/21335)为课程[CS 228](https://cs.stanford.edu/%257Eermon/cs228/index.html)创建的。

内容列表：

- Python

- - 基本数据类型

  - 容器

  - - 列表
    - 字典
    - 集合
    - 元组

  - 函数

  - 类

- Numpy

- - 数组
  - 访问数组
  - 数据类型
  - 数组计算
  - 广播

- SciPy

- - 图像操作
  - MATLAB文件
  - 点之间的距离

- Matplotlib

- - 绘制图形
  - 绘制多个图形
  - 图像

### Python

**Python是一种高级的，动态类型的多范型编程语言**。很多时候，大家会说Python看起来简直和伪代码一样，这是因为你能够通过很少行数的代码表达出很有力的思想。举个例子，下面是用Python实现的经典的quicksort算法例子：

```python
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) / 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)

print quicksort([3,6,8,10,1,2,1])
# Prints "[1, 1, 2, 3, 6, 8, 10]"
```

#### Python版本

Python有两个支持的版本，分别是2.7和3.4。这有点让人迷惑，3.0向语言中引入了很多不向后兼容的变化，2.7下的代码有时候在3.4下是行不通的。在这个课程中，我们使用的是2.7版本。

如何查看版本呢？使用**python --version**命令。

#### 基本数据类型

和大多数编程语言一样，Python拥有一系列的基本数据类型，比如整型、浮点型、布尔型和字符串等。这些类型的使用方式和在其他语言中的使用方式是类似的。

**数字**：整型和浮点型的使用与其他语言类似。

```python
x = 3
print type(x) # Prints "<type 'int'>"
print x       # Prints "3"
print x + 1   # Addition; prints "4"
print x - 1   # Subtraction; prints "2"
print x * 2   # Multiplication; prints "6"
print x ** 2  # Exponentiation; prints "9"
x += 1
print x  # Prints "4"
x *= 2
print x  # Prints "8"
y = 2.5
print type(y) # Prints "<type 'float'>"
print y, y + 1, y * 2, y ** 2 # Prints "2.5 3.5 5.0 6.25"
```

需要注意的是，==Python中没有 x++ 和 x-- 的操作符==。

Python也有内置的长整型和复杂数字类型，具体细节可以查看[文档](https://docs.python.org/2/library/stdtypes.html%23numeric-types-int-float-long-complex)。

**布尔型**：Python实现了所有的布尔逻辑，但用的是英语，而不是我们习惯的操作符（比如&&和||等）。

```python
t = True
f = False
print type(t) # Prints "<type 'bool'>"
print t and f # Logical AND; prints "False"
print t or f  # Logical OR; prints "True"
print not t   # Logical NOT; prints "False"
print t != f  # Logical XOR; prints "True"  
```

**字符串**：Python对字符串的支持非常棒。

```python
hello = 'hello'   # String literals can use single quotes
world = "world"   # or double quotes; it does not matter.
print hello       # Prints "hello"
print len(hello)  # String length; prints "5"
hw = hello + ' ' + world  # String concatenation
print hw  # prints "hello world"
hw12 = '%s %s %d' % (hello, world, 12)  # sprintf style string formatting
print hw12  # prints "hello world 12"
```

字符串对象有一系列有用的方法，比如：

```python
s = "hello"
print s.capitalize()  # Capitalize a string; prints "Hello"
print s.upper()       # Convert a string to uppercase; prints "HELLO"
print s.rjust(7)      # Right-justify a string, padding with spaces; prints "  hello"
print s.center(7)     # Center a string, padding with spaces; prints " hello "
print s.replace('l', '(ell)')  # Replace all instances of one substring with another;
                               # prints "he(ell)(ell)o"
print '  world '.strip()  # Strip leading and trailing whitespace; prints "world"
```

如果想详细查看字符串方法，请看[文档](https://docs.python.org/2/library/stdtypes.html%23string-methods)。

#### 容器Containers

**译者注**：有知友建议container翻译为复合数据类型，供读者参考。

Python有以下几种容器类型：列表（lists）、字典（dictionaries）、集合（sets）和元组（tuples）。

##### 列表Lists

列表就是Python中的数组，但是列表长度可变，且能包含不同类型元素。

```python
xs = [3, 1, 2]   # Create a list
print xs, xs[2]  # Prints "[3, 1, 2] 2"
print xs[-1]     # Negative indices count from the end of the list; prints "2"
xs[2] = 'foo'    # Lists can contain elements of different types
print xs         # Prints "[3, 1, 'foo']"
xs.append('bar') # Add a new element to the end of the list
print xs         # Prints
x = xs.pop()     # Remove and return the last element of the list
print x, xs      # Prints "bar [3, 1, 'foo']"
```

列表的细节，同样可以查阅[文档](https://docs.python.org/2/tutorial/datastructures.html%23more-on-lists)。

**切片Slicing**：为了一次性地获取列表中的元素，Python提供了一种简洁的语法，这就是切片。

```python
nums = range(5)    # range is a built-in function that creates a list of integers
print nums         # Prints "[0, 1, 2, 3, 4]"
print nums[2:4]    # Get a slice from index 2 to 4 (exclusive); prints "[2, 3]"
print nums[2:]     # Get a slice from index 2 to the end; prints "[2, 3, 4]"
print nums[:2]     # Get a slice from the start to index 2 (exclusive); prints "[0, 1]"
print nums[:]      # Get a slice of the whole list; prints ["0, 1, 2, 3, 4]"
print nums[:-1]    # Slice indices can be negative; prints ["0, 1, 2, 3]"
nums[2:4] = [8, 9] # Assign a new sublist to a slice
print nums         # Prints "[0, 1, 8, 9, 4]"
```

在Numpy数组的内容中，我们会再次看到切片语法。

**循环Loops**：我们可以这样遍历列表中的每一个元素：

```python
animals = ['cat', 'dog', 'monkey']
for animal in animals:
    print animal
# Prints "cat", "dog", "monkey", each on its own line.
```

如果想要在循环体内访问每个元素的指针，可以使用内置的**enumerate**函数

```python
animals = ['cat', 'dog', 'monkey']
for idx, animal in enumerate(animals):
    print '#%d: %s' % (idx + 1, animal)
# Prints "#1: cat", "#2: dog", "#3: monkey", each on its own line
```

**列表推导List comprehensions**：在编程的时候，我们常常想要将一种数据类型转换为另一种。下面是一个简单例子，将列表中的每个元素变成它的平方。

```python
nums = [0, 1, 2, 3, 4]
squares = []
for x in nums:
    squares.append(x ** 2)
print squares   # Prints [0, 1, 4, 9, 16]
```

使用列表推导，你就可以让代码简化很多：

```python
nums = [0, 1, 2, 3, 4]
squares = [x ** 2 for x in nums]
print squares   # Prints [0, 1, 4, 9, 16]
```

列表推导还可以包含条件：

```python
nums = [0, 1, 2, 3, 4]
even_squares = [x ** 2 for x in nums if x % 2 == 0]
print even_squares  # Prints "[0, 4, 16]"
```

##### 字典Dictionaries

```python
d = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data
print d['cat']       # Get an entry from a dictionary; prints "cute"
print 'cat' in d     # Check if a dictionary has a given key; prints "True"
d['fish'] = 'wet'    # Set an entry in a dictionary
print d['fish']      # Prints "wet"
# print d['monkey']  # KeyError: 'monkey' not a key of d
print d.get('monkey', 'N/A')  # Get an element with a default; prints "N/A"
print d.get('fish', 'N/A')    # Get an element with a default; prints "wet"
del d['fish']        # Remove an element from a dictionary
print d.get('fish', 'N/A') # "fish" is no longer a key; prints "N/A"
```

想要知道字典的其他特性，请查阅[文档](https://docs.python.org/2/library/stdtypes.html%23dict)。

**循环Loops**：在字典中，用键来迭代更加容易。

```python
d = {'person': 2, 'cat': 4, 'spider': 8}
for animal in d:
    legs = d[animal]
    print 'A %s has %d legs' % (animal, legs)
# Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs"
```

如果你想要访问键和对应的值，那就使用**iteritems**方法：

```python
d = {'person': 2, 'cat': 4, 'spider': 8}
for animal, legs in d.iteritems():
    print 'A %s has %d legs' % (animal, legs)
# Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs"
```

**字典推导Dictionary comprehensions**：和列表推导类似，但是允许你方便地构建字典。

```python
nums = [0, 1, 2, 3, 4]
even_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0}
print even_num_to_square  # Prints "{0: 0, 2: 4, 4: 16}"
```

##### 集合Sets

```python
animals = {'cat', 'dog'}
print 'cat' in animals   # Check if an element is in a set; prints "True"
print 'fish' in animals  # prints "False"
animals.add('fish')      # Add an element to a set
print 'fish' in animals  # Prints "True"
print len(animals)       # Number of elements in a set; prints "3"
animals.add('cat')       # Adding an element that is already in the set does nothing
print len(animals)       # Prints "3"
animals.remove('cat')    # Remove an element from a set
print len(animals)       # Prints "2"
```

和前面一样，要知道更详细的，查看[文档](https://docs.python.org/2/library/sets.html%23set-objects)。

**循环Loops**：在集合中循环的语法和在列表中一样，但是集合是无序的，所以你在访问集合的元素的时候，不能做关于顺序的假设。

```python
animals = {'cat', 'dog', 'fish'}
for idx, animal in enumerate(animals):
    print '#%d: %s' % (idx + 1, animal)
# Prints "#1: fish", "#2: dog", "#3: cat"
```

**集合推导Set comprehensions**：和字典推导一样，可以很方便地构建集合：

```python
from math import sqrt
nums = {int(sqrt(x)) for x in range(30)}
print nums  # Prints "set([0, 1, 2, 3, 4, 5])"
```

##### 元组Tuples

```python
d = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys
print d
t = (5, 6)       # Create a tuple
print type(t)    # Prints "<type 'tuple'>"
print d[t]       # Prints "5"
print d[(1, 2)]  # Prints "1"
```

[文档](https://docs.python.org/2/tutorial/datastructures.html%23tuples-and-sequences)有更多元组的信息。

#### 函数Functions

```python
def sign(x):
    if x > 0:
        return 'positive'
    elif x < 0:
        return 'negative'
    else:
        return 'zero'

for x in [-1, 0, 1]:
    print sign(x)
# Prints "negative", "zero", "positive"
```

我们常常使用可选参数来定义函数：

```python
def hello(name, loud=False):
    if loud:
        print 'HELLO, %s' % name.upper()
    else:
        print 'Hello, %s!' % name

hello('Bob') # Prints "Hello, Bob"
hello('Fred', loud=True)  # Prints "HELLO, FRED!"
```

函数还有很多内容，可以查看[文档](https://docs.python.org/2/tutorial/controlflow.html%23defining-functions)。

#### 类Classes

Python对于类的定义是简单直接的：

```python
class Greeter(object):

    # Constructor
    def __init__(self, name):
        self.name = name  # Create an instance variable

    # Instance method
    def greet(self, loud=False):
        if loud:
            print 'HELLO, %s!' % self.name.upper()
        else:
            print 'Hello, %s' % self.name

g = Greeter('Fred')  # Construct an instance of the Greeter class
g.greet()            # Call an instance method; prints "Hello, Fred"
g.greet(loud=True)   # Call an instance method; prints "HELLO, FRED!"
```

更多类的信息请查阅[文档](https://docs.python.org/2/tutorial/classes.html)。

### Numpy

Numpy是Python中用于科学计算的核心库。它提供了高性能的多维数组对象，以及相关工具。

#### 数组Arrays

一个numpy数组是一个由不同数值组成的网格。网格中的数据都是同一种数据类型，可以通过非负整型数的元组来访问。维度的数量被称为数组的阶，数组的大小是一个由整型数构成的元组，可以描述数组不同维度上的大小。

我们可以从列表创建数组，然后利用方括号访问其中的元素：

```python
import numpy as np

a = np.array([1, 2, 3])  # Create a rank 1 array
print type(a)            # Prints "<type 'numpy.ndarray'>"
print a.shape            # Prints "(3,)"
print a[0], a[1], a[2]   # Prints "1 2 3"
a[0] = 5                 # Change an element of the array
print a                  # Prints "[5, 2, 3]"

b = np.array([[1,2,3],[4,5,6]])   # Create a rank 2 array
print b                           # 显示一下矩阵b
print b.shape                     # Prints "(2, 3)"
print b[0, 0], b[0, 1], b[1, 0]   # Prints "1 2 4"
```

Numpy还提供了很多其他创建数组的方法：

```python
import numpy as np

a = np.zeros((2,2))  # Create an array of all zeros
print a              # Prints "[[ 0.  0.]
                     #          [ 0.  0.]]"

b = np.ones((1,2))   # Create an array of all ones
print b              # Prints "[[ 1.  1.]]"

c = np.full((2,2), 7) # Create a constant array
print c               # Prints "[[ 7.  7.]
                      #          [ 7.  7.]]"

d = np.eye(2)        # Create a 2x2 identity matrix
print d              # Prints "[[ 1.  0.]
                     #          [ 0.  1.]]"

e = np.random.random((2,2)) # Create an array filled with random values
print e                     # Might print "[[ 0.91940167  0.08143941]
                            #               [ 0.68744134  0.87236687]]"
```

其他数组相关方法，请查看[文档](https://docs.scipy.org/doc/numpy/user/basics.creation.html%23arrays-creation)。

#### 访问数组

Numpy提供了多种访问数组的方法。

**切片**：和Python列表类似，numpy数组可以使用切片语法。因为数组可以是多维的，所以你**必须**为每个维度指定好切片。

```python
import numpy as np

# Create the following rank 2 array with shape (3, 4)
# [[ 1  2  3  4]
#  [ 5  6  7  8]
#  [ 9 10 11 12]]
a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])

# Use slicing to pull out the subarray consisting of the first 2 rows
# and columns 1 and 2; b is the following array of shape (2, 2):
# [[2 3]
#  [6 7]]
b = a[:2, 1:3]

# A slice of an array is a view into the same data, so modifying it
# will modify the original array.
print a[0, 1]   # Prints "2"
b[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1]
print a[0, 1]   # Prints "77"
```

你可以同时使用整型和切片语法来访问数组。但是，这样做会产生一个比原数组低阶的新数组。需要注意的是，==这里和MATLAB中的情况是不同的==：

```python
import numpy as np

# Create the following rank 2 array with shape (3, 4)
# [[ 1  2  3  4]
#  [ 5  6  7  8]
#  [ 9 10 11 12]]
a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])

# Two ways of accessing the data in the middle row of the array.
# Mixing integer indexing with slices yields an array of lower rank,
# while using only slices yields an array of the same rank as the
# original array:
row_r1 = a[1, :]    # Rank 1 view of the second row of a  
row_r2 = a[1:2, :]  # Rank 2 view of the second row of a
print row_r1, row_r1.shape  # Prints "[5 6 7 8] (4,)"
print row_r2, row_r2.shape  # Prints "[[5 6 7 8]] (1, 4)"

# We can make the same distinction when accessing columns of an array:
col_r1 = a[:, 1]
col_r2 = a[:, 1:2]
print col_r1, col_r1.shape  # Prints "[ 2  6 10] (3,)"
print col_r2, col_r2.shape  # Prints "[[ 2]
                            #          [ 6]
                            #          [10]] (3, 1)"
```

**整型数组访问**：当我们使用切片语法访问数组时，得到的总是原数组的一个子集。整型数组访问允许我们利用其它数组的数据构建一个新的数组：

```python
import numpy as np

a = np.array([[1,2], [3, 4], [5, 6]])

# An example of integer array indexing.
# The returned array will have shape (3,) and
print a[[0, 1, 2], [0, 1, 0]]  # Prints "[1 4 5]"

# The above example of integer array indexing is equivalent to this:
print np.array([a[0, 0], a[1, 1], a[2, 0]])  # Prints "[1 4 5]"

# When using integer array indexing, you can reuse the same
# element from the source array:
print a[[0, 0], [1, 1]]  # Prints "[2 2]"

# Equivalent to the previous integer array indexing example
print np.array([a[0, 1], a[0, 1]])  # Prints "[2 2]"
```

整型数组访问语法还有个有用的技巧，可以用来选择或者更改矩阵中每行中的一个元素：

```python
import numpy as np

# Create a new array from which we will select elements
a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])

print a  # prints "array([[ 1,  2,  3],
         #                [ 4,  5,  6],
         #                [ 7,  8,  9],
         #                [10, 11, 12]])"

# Create an array of indices
b = np.array([0, 2, 0, 1])

# Select one element from each row of a using the indices in b
print a[np.arange(4), b]  # Prints "[ 1  6  7 11]"

# Mutate one element from each row of a using the indices in b
a[np.arange(4), b] += 10

print a  # prints "array([[11,  2,  3],
         #                [ 4,  5, 16],
         #                [17,  8,  9],
         #                [10, 21, 12]])
```

**布尔型数组访问**：布尔型数组访问可以让你选择数组中任意元素。通常，这种访问方式用于选取数组中满足某些条件的元素，举例如下：

```python
import numpy as np

a = np.array([[1,2], [3, 4], [5, 6]])

bool_idx = (a > 2)  # Find the elements of a that are bigger than 2;
                    # this returns a numpy array of Booleans of the same
                    # shape as a, where each slot of bool_idx tells
                    # whether that element of a is > 2.

print bool_idx      # Prints "[[False False]
                    #          [ True  True]
                    #          [ True  True]]"

# We use boolean array indexing to construct a rank 1 array
# consisting of the elements of a corresponding to the True values
# of bool_idx
print a[bool_idx]  # Prints "[3 4 5 6]"

# We can do all of the above in a single concise statement:
print a[a > 2]     # Prints "[3 4 5 6]"
```

为了教程的简介，有很多数组访问的细节我们没有详细说明，可以查看[文档](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)。

#### 数据类型

每个Numpy数组都是数据类型相同的元素组成的网格。Numpy提供了很多的数据类型用于创建数组。当你创建数组的时候，Numpy会尝试猜测数组的数据类型，你也可以通过参数直接指定数据类型，例子如下：

```python
import numpy as np

x = np.array([1, 2])  # Let numpy choose the datatype
print x.dtype         # Prints "int64"

x = np.array([1.0, 2.0])  # Let numpy choose the datatype
print x.dtype             # Prints "float64"

x = np.array([1, 2], dtype=np.int64)  # Force a particular datatype
print x.dtype                         # Prints "int64"
```

更多细节查看[文档](https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html)。

#### 数组计算

基本数学计算函数会对数组中元素逐个进行计算，既可以利用操作符重载，也可以使用函数方式：

```python
import numpy as np

x = np.array([[1,2],[3,4]], dtype=np.float64)
y = np.array([[5,6],[7,8]], dtype=np.float64)

# Elementwise sum; both produce the array
# [[ 6.0  8.0]
#  [10.0 12.0]]
print x + y
print np.add(x, y)

# Elementwise difference; both produce the array
# [[-4.0 -4.0]
#  [-4.0 -4.0]]
print x - y
print np.subtract(x, y)

# Elementwise product; both produce the array
# [[ 5.0 12.0]
#  [21.0 32.0]]
print x * y
print np.multiply(x, y)

# Elementwise division; both produce the array
# [[ 0.2         0.33333333]
#  [ 0.42857143  0.5       ]]
print x / y
print np.divide(x, y)

# Elementwise square root; produces the array
# [[ 1.          1.41421356]
#  [ 1.73205081  2.        ]]
print np.sqrt(x)
```

==和MATLAB不同，*是元素逐个相乘，而不是矩阵乘法。在Numpy中使用dot来进行矩阵乘法==：

```python
import numpy as np

x = np.array([[1,2],[3,4]])
y = np.array([[5,6],[7,8]])

v = np.array([9,10])
w = np.array([11, 12])

# Inner product of vectors; both produce 219
print v.dot(w)
print np.dot(v, w)

# Matrix / vector product; both produce the rank 1 array [29 67]
print x.dot(v)
print np.dot(x, v)

# Matrix / matrix product; both produce the rank 2 array
# [[19 22]
#  [43 50]]
print x.dot(y)
print np.dot(x, y)
```

Numpy提供了很多计算数组的函数，其中最常用的一个是$sum$：

```python
import numpy as np

x = np.array([[1,2],[3,4]])

print np.sum(x)  # Compute sum of all elements; prints "10"
print np.sum(x, axis=0)  # Compute sum of each column; prints "[4 6]"
print np.sum(x, axis=1)  # Compute sum of each row; prints "[3 7]"
```

想要了解更多函数，可以查看[文档](https://docs.scipy.org/doc/numpy/reference/routines.math.html)。

除了计算，我们还常常改变数组或者操作其中的元素。其中将矩阵转置是常用的一个，在Numpy中，使用$T$来转置矩阵：

```python
import numpy as np

x = np.array([[1,2], [3,4]])
print x    # Prints "[[1 2]
           #          [3 4]]"
print x.T  # Prints "[[1 3]
           #          [2 4]]"

# Note that taking the transpose of a rank 1 array does nothing:
v = np.array([1,2,3])
print v    # Prints "[1 2 3]"
print v.T  # Prints "[1 2 3]"
```

Numpy还提供了更多操作数组的方法，请查看[文档](https://docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html)。

#### 广播Broadcasting

广播是一种强有力的机制，它让Numpy可以让不同大小的矩阵在一起进行数学计算。我们常常会有一个小的矩阵和一个大的矩阵，然后我们会需要用小的矩阵对大的矩阵做一些计算。

举个例子，如果我们想要把一个向量加到矩阵的每一行，我们可以这样做：

```python
import numpy as np

# We will add the vector v to each row of the matrix x,
# storing the result in the matrix y
x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
v = np.array([1, 0, 1])
y = np.empty_like(x)   # Create an empty matrix with the same shape as x

# Add the vector v to each row of the matrix x with an explicit loop
for i in range(4):
    y[i, :] = x[i, :] + v

# Now y is the following
# [[ 2  2  4]
#  [ 5  5  7]
#  [ 8  8 10]
#  [11 11 13]]
print y
```

这样是行得通的，但是当$x$矩阵非常大，利用循环来计算就会变得很慢很慢。我们可以换一种思路：

```python
import numpy as np

# We will add the vector v to each row of the matrix x,
# storing the result in the matrix y
x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
v = np.array([1, 0, 1])
vv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other
print vv                 # Prints "[[1 0 1]
                         #          [1 0 1]
                         #          [1 0 1]
                         #          [1 0 1]]"
y = x + vv  # Add x and vv elementwise
print y  # Prints "[[ 2  2  4
         #          [ 5  5  7]
         #          [ 8  8 10]
         #          [11 11 13]]"
```

Numpy广播机制可以让我们不用创建vv，就能直接运算，看看下面例子：

```python
import numpy as np

# We will add the vector v to each row of the matrix x,
# storing the result in the matrix y
x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
v = np.array([1, 0, 1])
y = x + v  # Add v to each row of x using broadcasting
print y  # Prints "[[ 2  2  4]
         #          [ 5  5  7]
         #          [ 8  8 10]
         #          [11 11 13]]"
```

对两个数组使用广播机制要遵守下列规则：

1. 如果数组的秩不同，使用$1$来将秩较小的数组进行扩展，直到两个数组的尺寸的长度都一样。
2. 如果两个数组在某个维度上的长度是一样的，或者其中一个数组在该维度上长度为$1$，那么我们就说这两个数组在该维度上是**相容**的。
3. 如果两个数组在所有维度上都是相容的，他们就能使用广播。
4. 如果两个输入数组的尺寸不同，那么注意其中较大的那个尺寸。因为广播之后，两个数组的尺寸将和那个较大的尺寸一样。
5. 在任何一个维度上，如果一个数组的长度为$1$，另一个数组长度大于$1$，那么在该维度上，就好像是对第一个数组进行了复制。

如果上述解释看不明白，可以读一读[文档](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)和这个[解释](https://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc)。**译者注**：强烈推荐阅读文档中的例子。

支持广播机制的函数是全局函数。哪些是全局函数可以在[文档](https://docs.scipy.org/doc/numpy/reference/ufuncs.html%23available-ufuncs)中查找。

下面是一些广播机制的使用：

```python
import numpy as np

# Compute outer product of vectors
v = np.array([1,2,3])  # v has shape (3,)
w = np.array([4,5])    # w has shape (2,)
# To compute an outer product, we first reshape v to be a column
# vector of shape (3, 1); we can then broadcast it against w to yield
# an output of shape (3, 2), which is the outer product of v and w:
# [[ 4  5]
#  [ 8 10]
#  [12 15]]
print np.reshape(v, (3, 1)) * w

# Add a vector to each row of a matrix
x = np.array([[1,2,3], [4,5,6]])
# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),
# giving the following matrix:
# [[2 4 6]
#  [5 7 9]]
print x + v

# Add a vector to each column of a matrix
# x has shape (2, 3) and w has shape (2,).
# If we transpose x then it has shape (3, 2) and can be broadcast
# against w to yield a result of shape (3, 2); transposing this result
# yields the final result of shape (2, 3) which is the matrix x with
# the vector w added to each column. Gives the following matrix:
# [[ 5  6  7]
#  [ 9 10 11]]
print (x.T + w).T

# Another solution is to reshape w to be a row vector of shape (2, 1);
# we can then broadcast it directly against x to produce the same
# output.
print x + np.reshape(w, (2, 1))

# Multiply a matrix by a constant:
# x has shape (2, 3). Numpy treats scalars as arrays of shape ();
# these can be broadcast together to shape (2, 3), producing the
# following array:
# [[ 2  4  6]
#  [ 8 10 12]]
print x * 2
```

广播机制能够让你的代码更简洁更迅速，能够用的时候请尽量使用！

#### Numpy文档

这篇教程涉及了你需要了解的numpy中的一些重要内容，但是numpy远不止如此。可以查阅[numpy文献](https://docs.scipy.org/doc/numpy/reference/)来学习更多。

### SciPy

Numpy提供了高性能的多维数组，以及计算和操作数组的基本工具。[SciPy](https://docs.scipy.org/doc/scipy/reference/)基于Numpy，提供了大量的计算和操作数组的函数，这些函数对于不同类型的科学和工程计算非常有用。

熟悉SciPy的最好方法就是阅读[文档](https://docs.scipy.org/doc/scipy/reference/index.html)。我们会强调对于本课程有用的部分。

#### 图像操作

SciPy提供了一些操作图像的基本函数。比如，它提供了将图像从硬盘读入到数组的函数，也提供了将数组中数据写入的硬盘成为图像的函数。下面是一个简单的例子：

```python
from scipy.misc import imread, imsave, imresize

# Read an JPEG image into a numpy array
img = imread('assets/cat.jpg')
print img.dtype, img.shape  # Prints "uint8 (400, 248, 3)"

# We can tint the image by scaling each of the color channels
# by a different scalar constant. The image has shape (400, 248, 3);
# we multiply it by the array [1, 0.95, 0.9] of shape (3,);
# numpy broadcasting means that this leaves the red channel unchanged,
# and multiplies the green and blue channels by 0.95 and 0.9
# respectively.
img_tinted = img * [1, 0.95, 0.9]

# Resize the tinted image to be 300 by 300 pixels.
img_tinted = imresize(img_tinted, (300, 300))

# Write the tinted image back to disk
imsave('assets/cat_tinted.jpg', img_tinted)
```

**译者注**：如果运行这段代码出现类似**ImportError: cannot import name imread**的报错，那么请利用pip进行Pillow的下载，可以解决问题。命令：`pip install Pillow`。

—————————————————————————————————————————

![img](https://pic4.zhimg.com/ff5f35bdb1a53c5e8dd5a16b391f63df_b.png)

左边是原始图片，右边是变色和变形的图片。

—————————————————————————————————————————

#### MATLAB文件

函数**scipy.io.loadmat**和**scipy.io.savemat**能够让你读和写MATLAB文件。具体请查看[文档](https://docs.scipy.org/doc/scipy/reference/io.html)。

#### 点之间的距离

SciPy定义了一些有用的函数，可以计算集合中点之间的距离。

函数**scipy.spatial.distance.pdist**能够计算集合中所有两点之间的距离：

```python
import numpy as np
from scipy.spatial.distance import pdist, squareform

# Create the following array where each row is a point in 2D space:
# [[0 1]
#  [1 0]
#  [2 0]]
x = np.array([[0, 1], [1, 0], [2, 0]])
print x

# Compute the Euclidean distance between all rows of x.
# d[i, j] is the Euclidean distance between x[i, :] and x[j, :],
# and d is the following array:
# [[ 0.          1.41421356  2.23606798]
#  [ 1.41421356  0.          1.        ]
#  [ 2.23606798  1.          0.        ]]
d = squareform(pdist(x, 'euclidean'))
print d
```

具体细节请阅读[文档](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)。

函数**scipy.spatial.distance.cdist**可以计算不同集合中点的距离，具体请查看[文档](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html)。

### Matplotlib

Matplotlib是一个作图库。这里简要介绍**matplotlib.pyplot**模块，功能和MATLAB的作图功能类似。

#### 绘图

matplotlib库中最重要的函数是**Plot**。该函数允许你做出$2D$图形，如下：

```python
import numpy as np
import matplotlib.pyplot as plt

# Compute the x and y coordinates for points on a sine curve
x = np.arange(0, 3 * np.pi, 0.1)
y = np.sin(x)

# Plot the points using matplotlib
plt.plot(x, y)
plt.show()  # You must call plt.show() to make graphics appear.
```

运行上面代码会产生下面的作图：

—————————————————————————————————————————

![img](https://pic4.zhimg.com/47169e8280088396107e0c62d7689e07_b.png)—————————————————————————————————————————

只需要少量工作，就可以一次画不同的线，加上标签，坐标轴标志等。

```python
import numpy as np
import matplotlib.pyplot as plt

# Compute the x and y coordinates for points on sine and cosine curves
x = np.arange(0, 3 * np.pi, 0.1)
y_sin = np.sin(x)
y_cos = np.cos(x)

# Plot the points using matplotlib
plt.plot(x, y_sin)
plt.plot(x, y_cos)
plt.xlabel('x axis label')
plt.ylabel('y axis label')
plt.title('Sine and Cosine')
plt.legend(['Sine', 'Cosine'])
plt.show()
```

—————————————————————————————————————————

![img](https://pic3.zhimg.com/955a7bcd45981728e91693961c21fbae_b.png)—————————————————————————————————————————

可以在[文档](https://matplotlib.org/api/pyplot_api.html%23matplotlib.pyplot.plot)中阅读更多关于$plot$的内容。

#### 绘制多个图像

可以使用**subplot**函数来在一幅图中画不同的东西：

```python
import numpy as np
import matplotlib.pyplot as plt

# Compute the x and y coordinates for points on sine and cosine curves
x = np.arange(0, 3 * np.pi, 0.1)
y_sin = np.sin(x)
y_cos = np.cos(x)

# Set up a subplot grid that has height 2 and width 1,
# and set the first such subplot as active.
plt.subplot(2, 1, 1)

# Make the first plot
plt.plot(x, y_sin)
plt.title('Sine')

# Set the second subplot as active, and make the second plot.
plt.subplot(2, 1, 2)
plt.plot(x, y_cos)
plt.title('Cosine')

# Show the figure.
plt.show()
```

—————————————————————————————————————————

![img](https://pic1.zhimg.com/c2abf551074a0db7445067f460417a08_b.png)—————————————————————————————————————————

关于**subplot**的更多细节，可以阅读[文档](https://matplotlib.org/api/pyplot_api.html%23matplotlib.pyplot.subplot)。

#### 图像

你可以使用**imshow**函数来显示图像，如下所示：

```python
import numpy as np
from scipy.misc import imread, imresize
import matplotlib.pyplot as plt

img = imread('assets/cat.jpg')
img_tinted = img * [1, 0.95, 0.9]

# Show the original image
plt.subplot(1, 2, 1)
plt.imshow(img)

# Show the tinted image
plt.subplot(1, 2, 2)

# A slight gotcha with imshow is that it might give strange results
# if presented with data that is not uint8. To work around this, we
# explicitly cast the image to uint8 before displaying it.
plt.imshow(np.uint8(img_tinted))
plt.show()
```

—————————————————————————————————————————

![img](https://pic2.zhimg.com/81197033afe9b507dea565ed558a6239_b.png)—————————————————————————————————————————

---

## 原文：[[image classification notes\]](https://cs231n.github.io/classification)。

翻译：[图像分类笔记（上）](https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit)[（下）](https://zhuanlan.zhihu.com/p/20900216?refer=intelligentunit)。

> 该笔记是一篇介绍性教程，面向非计算机视觉领域的同学。教程将向同学们介绍图像分类问题和数据驱动方法，内容列表：
>
> 
>
> - 图像分类、数据驱动方法和流程
>
> - Nearest Neighbor分类器
>
> - - k-Nearest Neighbor *译者注：上篇翻译截止处*
>
> - 验证集、交叉验证集和超参数调参
>
> - Nearest Neighbor的优劣
>
> - 小结
>
> - 小结：应用kNN实践
>
> - 拓展阅读

##  图像分类笔记

### 图像分类

**目标**：这一节我们将介绍图像分类问题。所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。在后面的课程中，我们可以看到计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。

**例子**：以下图为例，图像分类模型读取该图片，并生成该图片属于集合$ \{cat, dog, hat, mug\}$中各个标签的概率。需要注意的是，对于计算机来说，图像是一个由数字组成的巨大的$3$维数组。在这个例子中，猫的图像大小是宽$248$像素，高$400$像素，有$3$个颜色通道，分别是红、绿和蓝（简称RGB）。如此，该图像就包含了$248 * 400 * 3=297600$个数字，每个数字都是在范围`0 ~ 255`之间的整型，其中$0$表示全黑，$255$表示全白。我们的任务就是把这些上百万的数字变成一个简单的标签，比如“猫”。

—————————————————————————————————————————
![img](https://pic2.zhimg.com/baab9e4b97aceb77ec70abeda6be022d_b.png)

图像分类的任务，就是对于一个给定的图像，预测它属于的那个分类标签（或者给出属于一系列不同标签的可能性）。图像是$3$维数组，数组元素是取值范围从$0$到$255$的整数。数组的尺寸是$宽度 * 高度 * 3$，其中这个$3$代表的是红、绿和蓝$3$个颜色通道。

—————————————————————————————————————————

**困难和挑战**：对于人来说，识别出一个像“猫”一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。我们在下面列举了计算机视觉算法在图像识别方面遇到的一些困难，要记住图像是以3维数组来表示的，数组中的元素是亮度值。

- **视角变化（Viewpoint variation）**：同一个物体，摄像机可以从多个角度来展现。
- **大小变化（Scale variation）**：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。
- **形变（Deformation）**：很多东西的形状并非一成不变，会有很大变化。
- **遮挡（Occlusion）**：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的。
- **光照条件（Illumination conditions）**：在像素层面上，光照的影响非常大。
- **背景干扰（Background clutter）**：物体可能混入背景之中，使之难以被辨认。
- **类内差异（Intra-class variation）**：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。

面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。

—————————————————————————————————————————![img](https://pic2.zhimg.com/1ee9457872f773d671dd5b225647ef45_b.jpg)—————————————————————————————————————————

**数据驱动方法**：如何写一个图像分类的算法呢？这和写个排序算法可是大不一样。怎么写一个从图像中认出猫的算法？搞不清楚。因此，与其在代码中直接写明各类物体到底看起来是什么样的，倒不如说我们采取的方法和教小孩儿看图识物类似：给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。这种方法，就是数据驱动方法。既然该方法的第一步就是收集已经做好分类标注的图片来作为训练集，那么下面就看看数据库到底长什么样：—————————————————————————————————————————

![img](https://pic1.zhimg.com/bbbfd2e6878d6f5d2a82f8239addbbc0_b.jpg)

一个有$4$个视觉分类的训练集。在实际中，我们可能有上千的分类，每个分类都有成千上万的图像。

—————————————————————————————————————————

**图像分类流程**。在课程视频中已经学习过，图像分类就是输入一个元素为像素值的数组，然后给它分配一个分类标签。完整流程如下：

- **输入**：输入是包含$N$个图像的集合，每个图像的标签是$K$种分类标签中的一种。这个集合称为训练集。
- **学习**：这一步的任务是使用训练集来学习每个类到底长什么样。一般该步骤叫做训练分类器或者学习一个模型。
- **评价**：让分类器来预测它未曾见过的图像的分类标签，并以此来评价分类器的质量。我们会把分类器预测的标签和图像真正的分类标签对比。毫无疑问，分类器预测的分类标签和图像真正的分类标签如果一致，那就是好事，这样的情况越多越好。

#### Nearest Neighbor分类器

作为课程介绍的第一个方法，我们来实现一个**Nearest Neighbor分类器**。虽然这个分类器和卷积神经网络没有任何关系，实际中也极少使用，但通过实现它，可以让读者对于解决图像分类问题的方法有个基本的认识。

**图像分类数据集：CIFAR-10**。一个非常流行的图像分类数据集是[CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html)。这个数据集包含了$60000$张$32 * 32$的小图像。每张图像都有$10$种分类标签中的一种。这$60000$张图像被分为包含$50000$张图像的训练集和包含$10000$张图像的测试集。在下图中你可以看见$10$个类的$10$张随机图片。    

—————————————————————————————————————————![img](https://pic1.zhimg.com/fff49fd8cec00f77f657a4c4a679b030_b.jpg)

**左边**：从CIFAR-10数据库来的样本图像。**右边**：第一列是测试图像，然后第一列的每个测试图像右边是使用Nearest Neighbor算法，根据像素差异，从训练集中选出的$10$张最类似的图片。—————————————————————————————————————————

假设现在我们有CIFAR-10的$50000$张图片（每种分类$5000$张）作为训练集，我们希望将余下的$10000$作为测试集并给他们打上标签。Nearest Neighbor算法将会拿着测试图片和训练集中每一张图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。上面右边的图片就展示了这样的结果。请注意上面$10$个分类中，只有$3$个是准确的。比如第$8$行中，马头被分类为一个红色的跑车，原因在于红色跑车的黑色背景非常强烈，所以这匹马就被错误分类为跑车了。

那么具体如何比较两张图片呢？在本例中，就是比较$32 * 32 * 3$的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量$I_1$和$I_2$，然后计算他们的$L_1$距离：

$d_1(I_1,I_2)=\sum_p|I^p_1-I^p_2|$

这里的求和是针对所有的像素。下面是整个比较流程的图例：                                                                                      —————————————————————————————————————————         ![img](https://pic2.zhimg.com/95cfe7d9efb83806299c218e0710a6c5_r.jpg)                              —————————————————————————————————————————

以图片中的一个颜色通道为例来进行说明。两张图片使用$L_1$距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么$L_1$距离为$0$，但是如果两张图片很是不同，那$L_1$值将会非常大。

—————————————————————————————————————————

下面，让我们看看如何用代码来实现这个分类器。首先，我们将CIFAR-10的数据加载到内存中，并分成4个数组：训练数据和标签，测试数据和标签。在下面的代码中，$X_{tr}$大小是$50000  * 32 * 32 * 3$）存有训练集中所有的图像，$Y_{tr}$是对应的长度为$50000$的$1$维数组，存有图像对应的分类标签（从$0$到$9$）：

```python
Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide
# flatten out all images to be one-dimensional
Xtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072
Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072
```

现在我们得到所有的图像数据，并且把他们拉长成为行向量了。接下来展示如何训练并评价一个分类器：

```python
nn = NearestNeighbor() # create a Nearest Neighbor classifier class
nn.train(Xtr_rows, Ytr) # train the classifier on the training images and labels
Yte_predict = nn.predict(Xte_rows) # predict labels on the test images
# and now print the classification accuracy, which is the average number
# of examples that are correctly predicted (i.e. label matches)
print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )
```

作为评价标准，我们常常使用**准确率**，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个API：`train(X, y)`函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个`predict(X)`函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是使用$L_1$距离的Nearest Neighbor分类器的实现套路：

```python
import numpy as np

class NearestNeighbor(object):
  def __init__(self):
    pass

  def train(self, X, y):
    """ X is N x D where each row is an example. Y is 1-dimension of size N """
    # the nearest neighbor classifier simply remembers all the training data
    self.Xtr = X
    self.ytr = y

  def predict(self, X):
    """ X is N x D where each row is an example we wish to predict label for """
    num_test = X.shape[0]
    # lets make sure that the output type matches the input type
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

    # loop over all test rows
    for i in xrange(num_test):
      # find the nearest training image to the i'th test image
      # using the L1 distance (sum of absolute value differences)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
      min_index = np.argmin(distances) # get the index with smallest distance
      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example

    return Ypred
```

如果你用这段代码跑CIFAR-10，你会发现准确率能达到`38.6%`。这比随机猜测的`10%`要好，但是比人类识别的水平（[据研究推测是94%](http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/)）和卷积神经网络能达到的`95%`还是差多了。点击查看基于CIFAR-10数据的[Kaggle算法竞赛排行榜](http://www.kaggle.com/c/cifar-10/leaderboard)。

**距离选择**：计算向量间的距离有很多种方法，另一个常用的方法是$L_2$距离，从几何学的角度，可以理解为它在计算两个向量间的欧式距离。$L_2$距离的公式如下：                                                            

$d_2(I_1,I_2)=\sqrt{ \sum_p(I^p_1-I^p_2)^2}$

换句话说，我们依旧是在计算像素间的差值，只是先求其平方，然后把这些平方全部加起来，最后对这个和开方。在Numpy中，我们只需要替换上面代码中的1行代码就行：

```python
distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))
```

注意在这里使用了`np.sqrt`，但是在实际中可能不用。因为求平方根函数是一个单调函数，它对不同距离的绝对值求平方根虽然改变了数值大小，但依然保持了不同距离大小的顺序。所以用不用它，都能够对像素差异的大小进行正确比较。如果你在CIFAR-10上面跑这个模型，正确率是`35.4%`，比刚才低了一点。

**$L_1$和$L_2$比较**。比较这两个度量方式是挺有意思的。在面对两个向量之间的差异时，$L_2$比$L_1$更加不能容忍这些差异。也就是说，相对于$1$个巨大的差异，$L_2$距离更倾向于接受多个中等程度的差异。$L_1$和$L_2$都是在[p-norm](http://planetmath.org/vectorpnorm)常用的特殊形式。

#### k-Nearest Neighbor分类器

你可能注意到了，为什么只用最相似的$1$张图片的标签来作为测试图像的标签呢？这不是很奇怪吗！是的，使用k-Nearest Neighbor分类器就能做得更好。它的思想很简单：与其只找最相近的那$1$个图片的标签，我们找最相似的$k$个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当$k=1$的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。从直观感受上就可以看到，更高的k值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。—————————————————————————————————————————![img](https://pic3.zhimg.com/51aef845faa10195e33bdd4657592f86_b.jpg)

上面示例展示了Nearest Neighbor分类器和5-Nearest Neighbor分类器的区别。例子使用了$2$维的点来表示，分成$3$类（红、蓝和绿）。不同颜色区域代表的是使用$L_2$距离的分类器的决策边界。白色的区域是分类模糊的例子（即图像与两个以上的分类标签绑定）。需要注意的是，在NN分类器中，异常的数据点（比如：在蓝色区域中的绿点）制造出一个不正确预测的孤岛。5-NN分类器将这些不规则都平滑了，使得它针对测试数据的泛化（generalization）能力更好（例子中未展示）。注意，5-NN中也存在一些灰色区域，这些区域是因为近邻标签的最高票数相同导致的（比如：$2$个邻居是红色，$2$个邻居是蓝色，还有$1$个是绿色）。—————————————————————————————————————————

在实际中，大多使用k-NN分类器。但是$k$值如何确定呢？接下来就讨论这个问题。

#### 用于超参数调优的验证集

k-NN分类器需要设定$k$值，那么选择哪个$k$值最合适的呢？我们可以选择不同的距离函数，比如$L_1$范数和$L_2$范数等，那么选哪个好？还有不少选择我们甚至连考虑都没有考虑到（比如：点积）。所有这些选择，被称为**超参数（hyperparameter）**。在基于数据进行学习的机器学习算法设计中，超参数是很常见的。一般说来，这些超参数具体怎么设置或取值并不是显而易见的。

你可能会建议尝试不同的值，看哪个值表现最好就选哪个。好主意！我们就是这么做的，但这样做的时候要非常细心。特别注意：决不能使用测试集来进行调优。当你在设计机器学习算法的时候，应该把测试集看做非常珍贵的资源，不到最后一步，绝不使用它。如果你使用测试集来调优，而且算法看起来效果不错，那么真正的危险在于：算法实际部署后，性能可能会远低于预期。这种情况，称之为算法对测试集过拟合。从另一个角度来说，如果使用测试集来调优，实际上就是把测试集当做训练集，由测试集训练出来的算法再跑测试集，自然性能看起来会很好。这其实是过于乐观了，实际部署起来效果就会差很多。所以，最终测试的时候再使用测试集，可以很好地近似度量你所设计的分类器的泛化性能（在接下来的课程中会有很多关于泛化性能的讨论）。

> 测试数据集只使用一次，即在训练完成后评价最终的模型时使用。

好在我们有不用测试集调优的方法。其思路是：从训练集中取出一部分数据用来调优，我们称之为验证集（validation set）。以CIFAR-10为例，我们可以用$49000$个图像作为训练集，用$1000$个图像作为验证集。验证集其实就是作为假的测试集来调优。下面就是代码：

```python
# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before
# recall Xtr_rows is 50,000 x 3072 matrix
Xval_rows = Xtr_rows[:1000, :] # take first 1000 for validation
Yval = Ytr[:1000]
Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for train
Ytr = Ytr[1000:]

# find hyperparameters that work best on the validation set
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:

  # use a particular value of k and evaluation on validation data
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  # here we assume a modified NearestNeighbor class that can take a k as input
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
  print 'accuracy: %f' % (acc,)

  # keep track of what works on the validation set
  validation_accuracies.append((k, acc))
```

程序结束后，我们会作图分析出哪个$k$值表现最好，然后用这个$k$值来跑真正的测试集，并作出对算法的评价。

> 把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果。

**交叉验证**。有时候，训练集数量较小（因此验证集的数量更小），人们会使用一种被称为交叉验证的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取$1000$个图像，而是将训练集平均分成$5$份，其中$4$份用来训练，$1$份用来验证。然后我们循环着取其中$4$份来训练，其中$1$份来验证，最后取所有$5$次验证结果的平均值作为算法验证结果。

其中$4$份来训练，其中$1$份来验证，最后取所有$5$次验证结果的平均值作为算法验证结果。————————————————————————————————————————— ![img](https://pic1.zhimg.com/6a3ceec60cc0a379b4939c37ee3e89e8_r.png)

这就是$5$份交叉验证对$k$值调优的例子。针对每个$k$值，得到$5$个准确率结果，取其平均值，然后对不同k值的平均表现画线连接。本例中，当$k=7$的时算法表现最好（对应图中的准确率峰值）。如果我们将训练集分成更多份数，直线一般会更加平滑（噪音更少）。

—————————————————————————————————————————

**实际应用**。在实际情况下，人们不是很喜欢用交叉验证，主要是因为它会耗费较多的计算资源。一般直接把训练集按照`50% ~ 90%`的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成$3、5$和$10$份。————————————————————————————————————————— ![img](https://pic1.zhimg.com/cc88207c6c3c5e91df8b6367368f6450_r.jpg)

常用的数据分割模式。给出训练集和测试集后，训练集一般会被均分。这里是分成5份。前面4份用来训练，黄色那份用作验证集调优。如果采取交叉验证，那就各份轮流作为验证集。最后模型训练完毕，超参数都定好了，让模型跑一次（而且只跑一次）测试集，以此测试结果评价算法。

—————————————————————————————————————————

#### Nearest Neighbor分类器的优劣

现在对Nearest Neighbor分类器的优缺点进行思考。首先，Nearest Neighbor分类器易于理解，实现简单。其次，算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来。然而测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率。其实，我们后续要学习的卷积神经网络在这个权衡上走到了另一个极端：虽然训练花费很多时间，但是一旦训练完成，对新的测试数据进行分类非常快。这样的模式就符合实际使用需求。

Nearest Neighbor分类器的计算复杂度研究是一个活跃的研究领域，若干Approximate Nearest Neighbor (ANN)算法和库的使用可以提升Nearest Neighbor分类器在数据上的计算速度（比如：FLANN）。这些算法可以在准确率和时空复杂度之间进行权衡，并通常依赖一个预处理/索引过程，这个过程中一般包含kd树的创建和k-means算法的运用。Nearest Neighbor分类器在某些特定情况（比如数据维度较低）下，可能是不错的选择。但是在实际的图像分类工作中，很少使用。因为图像都是高维度数据（他们通常包含很多像素），而高维度向量之间的距离通常是反直觉的。下面的图片展示了基于像素的相似和基于感官的相似是有很大不同的：

—————————————————————————————————————————![img](https://pic3.zhimg.com/fd42d369eebdc5d81c89593ec1082e32_b.png)

在高维度数据上，基于像素的的距离和感官上的非常不同。上图中，右边3张图片和左边第1张原始图片的$L_2$距离是一样的。很显然，基于像素比较的相似和感官上以及语义上的相似是不同的。

—————————————————————————————————————————

这里还有个视觉化证据，可以证明使用像素差异来比较图像是不够的。$z$这是一个叫做[t-SNE](http://lvdmaaten.github.io/tsne/)的可视化技术，它将CIFAR-10中的图片按照二维方式排布，这样能很好展示图片之间的像素差异值。在这张图片中，排列相邻的图片$L_2$距离就小。

————————————————————————————————————————— ![img](https://pic1.zhimg.com/0f4980edb8710eaba0f3e661b1cbb830_r.jpg)

上图使用t-SNE的可视化技术将CIFAR-10的图片进行了二维排列。排列相近的图片$L_2$距离小。可以看出，图片的排列是被背景主导而不是图片语义内容本身主导。

——————————————————————————————————————————

具体说来，这些图片的排布更像是一种颜色分布函数，或者说是基于背景的，而不是图片的语义主体。比如，狗的图片可能和青蛙的图片非常接近，这是因为两张图片都是白色背景。从理想效果上来说，我们肯定是希望同类的图片能够聚集在一起，而不被背景或其他不相关因素干扰。为了达到这个目的，我们不能止步于原始像素比较，得继续前进。

#### 小结

简要说来：

- 介绍了**图像分类**问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。
- 介绍了一个简单的图像分类器：**最近邻分类器(Nearest Neighbor classifier)**。分类器中存在不同的超参数(比如k值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。
- 选取超参数的正确方法是：将原始训练集分为训练集和**验证集**，我们在验证集上尝试不同的超参数，最后保留表现最好那个。
- 如果训练数据量不够，使用**交叉验证**方法，它能帮助我们在选取最优超参数的时候减少噪音。
- 一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。
- 最近邻分类器能够在CIFAR-10上得到将近40%的准确率。该算法简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力。
- 最后，我们知道了仅仅使用$L_1$和$L_2$范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。

在接下来的课程中，我们将专注于解决这些问题和挑战，并最终能够得到超过`90%`准确率的解决方案。该方案能够在完成学习就丢掉训练集，并在一毫秒之内就完成一张图片的分类。

#### 实际应用k-NN

如果你希望将k-NN分类器用到实处（最好别用到图像上，若是仅仅作为练手还可以接受），那么可以按照以下流程：

1. 预处理你的数据：对你数据中的特征进行归一化（normalize），让其具有零平均值（zero mean）和单位方差（unit variance）。在后面的小节我们会讨论这些细节。本小节不讨论，是因为图像中的像素都是同质的，不会表现出较大的差异分布，也就不需要标准化处理了。
2. 如果数据是高维数据，考虑使用降维方法，比如PCA ([wiki ref](http://en.wikipedia.org/wiki/Principal_component_analysis), [CS229ref](http://cs229.stanford.edu/notes/cs229-notes10.pdf), [blog ref](http://www.bigdataexaminer.com/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/))或[随机投影](http://scikit-learn.org/stable/modules/random_projection.html)。
3. 将数据随机分入训练集和验证集。按照一般规律，`70% ~ 90%` 数据作为训练集。这个比例根据算法中有多少超参数，以及这些超参数对于算法的预期影响来决定。如果需要预测的超参数很多，那么就应该使用更大的验证集来有效地估计它们。如果担心验证集数量不够，那么就尝试交叉验证方法。如果计算资源足够，使用交叉验证总是更加安全的（份数越多，效果越好，也更耗费计算资源）。
4. 在验证集上调优，尝试足够多的$k$值，尝试$L_1$和$L_2$两种范数计算方式。
5. 如果分类器跑得太慢，尝试使用Approximate Nearest Neighbor库（比如[FLANN](http://www.cs.ubc.ca/research/flann/)）来加速这个过程，其代价是降低一些准确率。
6. 对最优的超参数做记录。记录最优参数后，是否应该让使用最优参数的算法在完整的训练集上运行并再次训练呢？因为如果把验证集重新放回到训练集中（自然训练集的数据量就又变大了），有可能最优参数又会有所变化。**在实践中，不要这样做**。千万不要在最终的分类器中使用验证集数据，这样做会破坏对于最优参数的估计。==直接使用测试集来测试用最优参数设置好的最优模型==，得到测试集数据的分类准确率，并以此作为你的kNN分类器在该数据上的性能表现。

#### 拓展阅读

下面是一些你可能感兴趣的拓展阅读链接：                                                            

- [A Few Useful Things to Know about Machine Learning](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)，文中第6节与本节相关，但是整篇文章都强烈推荐。
- [Recognizing and Learning Object Categories](http://people.csail.mit.edu/torralba/shortCourseRLOC/index.html)，ICCV 2005上的一节关于物体分类的课程。

---

## 原文：[[linear classification notes\]](https://cs231n.github.io/linear-classify)。

翻译：线性分类笔记[（上）](https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit)[（中）](https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit)[（下）](https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit)。

> 我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：一个是**评分函数（score function）**，它是原始图像数据到类别分值的映射。另一个是**损失函数（loss function）**，它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。内容列表：
> 线性分类器简介线性评分函数阐明线性分类器 *译者注：上篇翻译截止处*损失函数多类SVMSoftmax分类器SVM和Softmax的比较基于Web的可交互线性分类器原型小结

### 线性分类

上一篇笔记介绍了图像分类问题。图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。我们还介绍了k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor分类器存在以下不足：

- 分类器必须*记住*所有训练数据并将其存储起来，以便于未来测试数据用于比较。这在存储空间上是低效的，数据集的大小很容易就以GB计。
- 对一个测试图像进行分类需要和所有训练图像作比较，算法计算资源耗费高。

**概述**：我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：一个是**评分函数（score function）**，它是原始图像数据到类别分值的映射。另一个是**损失函数（loss function）**，它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。

### 从图像到标签分值的参数化映射

该方法的第一部分就是定义一个评分函数，这个函数将图像的像素值映射为各个分类类别的得分，得分高低代表图像属于该类别的可能性高低。下面会利用一个具体例子来展示该方法。现在假设有一个包含很多图像的训练集$x_i\in R^D$，每个图像都有一个对应的分类标签$y_i$。这里$i=1,2...N$并且$y_i\in 1...K$。这就是说，我们有$N$个图像样例，每个图像的维度是$D$，共有$K$种不同的分类。

举例来说，在CIFAR-10中，我们有一个$N=50000$的训练集，每个图像有$D=32*32*3=3072$个像素，而$K=10$，这是因为图片被分为$10$个不同的类别（狗，猫，汽车等）。我们现在定义评分函数为：$f:R^D\to R^K$，该函数是原始图像像素到分类分值的映射。

**线性分类器**：在本模型中，我们从最简单的概率函数开始，一个线性映射：
$$f(x_i,W,b)=Wx_i+b$$
在上面的公式中，假设每个图像数据都被拉长为一个长度为$D$的列向量，大小为$[D * 1]$。其中大小为$[K * D]$的矩阵$W$和大小为$[K * 1]$列向量$b$为该函数的**参数（parameters）**。还是以CIFAR-10为例，$x_i$就包含了第$i$个图像的所有像素信息，这些信息被拉成为一个$[3072  * 1]$的列向量，**W**大小为$[10 * 3072]$，**b**的大小为$[10 * 1]$。因此，$3072$个数字（原始像素数值）输入函数，函数输出$10$个数字（不同分类得到的分值）。参数$W$被称为**权重（weights）**。$b$被称为**偏差向量（bias vector）**，这是因为它影响输出数值，但是并不和原始数据$x_i$产生关联。在实际情况中，人们常常混用**权重**和**参数**这两个术语。

需要注意的几点：

- 首先，一个单独的矩阵乘法$Wx_i$就高效地并行评估$10$个不同的分类器（每个分类器针对一个分类），其中每个类的分类器就是$W$的一个行向量。
- 注意我们认为输入数据$(x_i,y_i)$是给定且不可改变的，但参数$W$和$b$是可控制改变的。我们的目标就是通过设置这些参数，使得计算出来的分类分值情况和训练集中图像数据的真实类别标签相符。在接下来的课程中，我们将详细介绍如何做到这一点，但是目前只需要直观地让正确分类的分值比错误分类的分值高即可。
- 该方法的一个优势是训练数据是用来学习到参数$W$和$b$的，一旦训练完成，训练数据就可以丢弃，留下学习到的参数即可。这是因为一个测试图像可以简单地输入函数，并基于计算出的分类分值来进行分类。
- 最后，注意只需要做一个矩阵乘法和一个矩阵加法就能对一个测试数据分类，这比k-NN中将测试图像和所有训练数据做比较的方法快多了。

> *预告：卷积神经网络映射图像像素值到分类分值的方法和上面一样，但是映射($f$)就要复杂多了，其包含的参数也更多。*

### 理解线性分类器

线性分类器计算图像中$3$个颜色通道中所有像素的值与权重的矩阵乘，从而得到分类分值。根据我们对权重设置的值，对于图像中的某些位置的某些颜色，函数表现出喜好或者厌恶（根据每个权重的符号而定）。举个例子，可以想象“船”分类就是被大量的蓝色所包围（对应的就是水）。那么“船”分类器在蓝色通道上的权重就有很多的正权重（它们的出现提高了“船”分类的分值），而在绿色和红色通道上的权重为负的就比较多（它们的出现降低了“船”分类的分值）。

————————————————————————————————————————![img](https://pic3.zhimg.com/7c204cd1010c0af1e7b50000bfff1d8e_b.jpg)

一个将图像映射到分类分值的例子。为了便于可视化，假设图像只有$4$个像素（都是黑白像素，这里不考虑RGB通道），有$3$个分类（红色代表猫，绿色代表狗，蓝色代表船，注意，这里的红、绿和蓝$3$种颜色仅代表分类，和RGB通道没有关系）。首先将图像像素拉伸为一个列向量，与W进行矩阵乘，然后得到各个分类的分值。需要注意的是，这个W一点也不好：猫分类的分值非常低。从上图来看，算法倒是觉得这个图像是一只狗。

————————————————————————————————————————

**将图像看做高维度的点**：既然图像被伸展成为了一个高维度的列向量，那么我们可以把图像看做这个高维度空间中的一个点（即每张图像是$3072$维空间中的一个点）。整个数据集就是一个点的集合，每个点都带有1个分类标签。

既然定义每个分类类别的分值是权重和图像的矩阵乘，那么每个分类类别的分数就是这个空间中的一个线性函数的函数值。我们没办法可视化$3072$维空间中的线性函数，但假设把这些维度挤压到二维，那么就可以看看这些分类器在做什么了：

——————————————————————————————————————————![img](https://pic2.zhimg.com/cfcb46408daa5353c38cb37e9bb6eb01_b.jpg)

图像空间的示意图。其中每个图像是一个点，有$3$个分类器。以红色的汽车分类器为例，红线表示空间中汽车分类分数为$0$的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低。

—————————————————————————————————————————

从上面可以看到，$W$的每一行都是一个分类类别的分类器。对于这些数字的几何解释是：如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转。而偏差$b$，则允许分类器对应的直线平移。需要注意的是，如果没有偏差，无论权重如何，在$x_i=0$时分类分值始终为$0$。这样所有分类器的线都不得不穿过原点。

**将线性分类器看做模板匹配**：关于权重$W$的另一个解释是它的每一行对应着一个分类的模板（有时候也叫作*原型*）。一张图像对应不同分类的得分，是通过使用内积（也叫*点积*）来比较图像和模板，然后找到和哪个模板最相似。从这个角度来看，线性分类器就是在利用学习到的模板，针对图像做模板匹配。从另一个角度来看，可以认为还是在高效地使用k-NN，不同的是我们没有使用所有的训练集的图像来比较，而是每个类别只用了一张图片（这张图片是我们学习到的，而不是训练集中的某一张），而且我们会使用（负）内积来计算向量间的距离，而不是使用$L1$或者$L2$距离。

————————————————————————————————————————

![img](https://pic1.zhimg.com/13e72e4ce83c11b49d36bbbb51d29ab4_b.jpg)

将课程进度快进一点。这里展示的是以CIFAR-10为训练集，学习结束后的权重的例子。注意，船的模板如期望的那样有很多蓝色像素。如果图像是一艘船行驶在大海上，那么这个模板利用内积计算图像将给出很高的分数。
————————————————————————————————————————

可以看到马的模板看起来似乎是两个头的马，这是因为训练集中的马的图像中马头朝向各有左右造成的。线性分类器将这两种情况融合到一起了。类似的，汽车的模板看起来也是将几个不同的模型融合到了一个模板中，并以此来分辨不同方向不同颜色的汽车。这个模板上的车是红色的，这是因为CIFAR-10中训练集的车大多是红色的。线性分类器对于不同颜色的车的分类能力是很弱的，但是后面可以看到神经网络是可以完成这一任务的。神经网络可以在它的隐藏层中实现中间神经元来探测不同种类的车（比如绿色车头向左，蓝色车头向前等）。而下一层的神经元通过计算不同的汽车探测器的权重和，将这些合并为一个更精确的汽车分类分值。

**偏差和权重的合并技巧**：在进一步学习前，要提一下这个经常使用的技巧。它能够将我们常用的参数$W$和$b$合二为一。回忆一下，分类评分函数定义为：

$f(x_i,W,b)=Wx_i+b$

分开处理这两个参数（权重参数$W$和偏差参数$b$）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时向量就要增加一个维度，这个维度的数值是常量$1$，这就是默认的偏差维度。这样新的公式就简化成下面这样：

$f(x_i,W)=Wx_i$

还是以CIFAR-10为例，那么$x_i$的大小就变成$[3073 * 1]$，而不是$[3072 * 1]$了，多出了包含常量$1$的$1$个维度）。$W$大小就是$[ 10 * 3073]$了。$W$中多出来的这一列对应的就是偏差值$b$，具体见下图：

————————————————————————————————————————

![img](https://pic2.zhimg.com/3c69a5c87a43bfb07e2b59bfcbd2f149_b.jpg)

偏差技巧的示意图。左边是先做矩阵乘法然后做加法，右边是将所有输入向量的维度增加$1$个含常量$1$的维度，并且在权重矩阵中增加一个偏差列，最后做一个矩阵乘法即可。左右是等价的。通过右边这样做，我们就只需要学习一个权重矩阵，而不用去学习两个分别装着权重和偏差的矩阵了。

—————————————————————————————————————————

**图像数据预处理**：在上面的例子中，所有图像都是使用的原始像素值（从$0$到$255$）。在机器学习中，对于输入的特征做归一化（normalization）处理是常见的套路。而在图像分类的例子中，图像上的每个像素可以看做一个特征。在实践中，对每个特征减去平均值来**中心化**数据是非常重要的。在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在$[-127, 127]$之间了。下一个常见步骤是，让所有数值分布的区间变为$[-1, 1]$。**零均值的中心化**是很重要的，等我们理解了梯度下降后再来详细解释。

### 损失函数 Loss function

在上一节定义了从图像像素值到所属类别的评分函数（score function），该函数的参数是权重矩阵。在函数中，数据是给定的，不能修改。但是我们可以调整权重矩阵这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（$score$）。

回到之前那张猫的图像分类例子，它有针对“猫”，“狗”，“船”三个类别的分数。我们看到例子中权重值非常差，因为猫分类的得分非常低（$-96.8$），而狗（$437.9$）和船（$61.95$）比较高。我们将使用**损失函数（Loss Function）**（有时也叫**代价函数Cost Function**或**目标函数Objective**）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。

#### 多类支持向量机损失 Multiclass Support Vector Machine Loss

损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值。我们可以把损失函数想象成一个人，这位SVM先生（或者女士）对于结果有自己的品位，如果某个结果能使得损失值更低，那么SVM就更加喜欢它。

让我们更精确一些。回忆一下，第i个数据中包含图像$x_i$的像素和代表正确类别的标签。评分函数输入像素数据，然后通过公式$f(x_i,W)$来计算不同分类类别的分值。这里我们将分值简写为。比如，针对第j个类别的得分就是第j个元素：$s_j=f(x_i,W)_j$。针对第i个数据的多类SVM的损失函数定义如下：

$\displaystyle L_i=\sum_{j\not=y_i}max(0,s_j-s_{y_i}+\Delta)$

**举例**：用一个例子演示公式是如何计算的。假设有$3$个分类，并且得到了分值$s=[13,-7,11]$。其中第一个类别是正确类别，即$y_i=0$。同时假设$\Delta$是$10$（后面会详细介绍该超参数）。上面的公式是将所有不正确分类（$j\not=y_i$）加起来，所以我们得到两个部分：
$Li=max(0,-7-13+10)+max(0,11-13+10)$

可以看到第一个部分结果是$0$，这是因为$[-7-13+10]$得到的是负数，经过$max(0,-)$函数处理后得到$0$。这一对类别分数和标签的损失值是$0$，这是因为正确分类的得分$13$与错误分类的得分$-7$的差为$20$，高于边界值$10$。而SVM只关心差距至少要大于$10$，更大的差值还是算作损失值为$0$。第二个部分计算$[11-13+10]$得到$8$。虽然正确分类的得分比不正确分类的得分要高（$13>11$），但是比$10$的边界值还是小了，分差只有$2$，这就是为什么损失值等于$8$。简而言之，SVM的损失函数想要正确分类类别的分数比不正确类别分数高，而且至少要高。如果不满足这点，就开始计算损失值。

那么在这次的模型中，我们面对的是线性评分函数（$f(x_i,W)=Wx_i$），所以我们可以将损失函数的公式稍微改写一下：

$\displaystyle L_i=\sum_{j\not=y_i}max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)$

其中$w_j$是权重$W$的第j行，被变形为列向量。然而，一旦开始考虑更复杂的评分函数$f$公式，这样做就不是必须的了。

在结束这一小节前，还必须提一下的属于是关于$0$的阀值：$max(0,-)$函数，它常被称为**折叶损失（hinge loss）**。有时候会听到人们使用平方折叶损失SVM（即L2-SVM），它使用的是 $max(0,-)^2$，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。

> 我们对于预测训练集数据分类标签的情况总有一些不满意的，而损失函数就能将这些不满意的程度量化。

—————————————————————————————————————————

![img](https://pic1.zhimg.com/f254bd8d072128f1088c8cc47c3dff58_r.jpg)

多类SVM“想要”正确类别的分类分数比其他不正确分类类别的分数要高，而且至少高出$delta$的边界值。如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为$0$。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。

—————————————————————————————————————————

**正则化（Regularization）**：上面损失函数有一个问题。假设有一个数据集和一个权重集$W$能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有$L_i=0$）。问题在于这个$W$并不唯一：可能有很多相似的$W$都能正确地分类所有的数据。一个简单的例子：如果$W$能够正确分类所有数据，即对于每个数据，损失值都是$0$。那么当 $\lambda>1$时，任何数乘都能使得损失值为$0$，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是$15$，对$W$乘以$2$将使得差距变成$30$。

换句话说，我们希望能向某些特定的权重W添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个**正则化惩罚（regularization penalty）**$R(W)$部分。最常用的正则化惩罚是$L2$范式，$L2$范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：

$\displaystyle R(W)=\sum_k \sum_l W^2_{k,l}$

上面的表达式中，将$W$中所有元素平方后求和。注意正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类SVM损失函数了，它由两个部分组成：**数据损失（data loss）**，即所有样例的的平均损失$L_i$，以及**正则化损失（regularization loss）**。完整公式如下所示：

$L=\displaystyle \underbrace{ \frac{1}{N}\sum_i L_i}_{data \  loss}+\underbrace{\lambda R(W)}_{regularization \ loss}$

将其展开完整公式是：

$\displaystyle L=\frac{1}{N}\sum_i\sum_{j\not=y_i}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+\Delta)]+\lambda \sum_k \sum_l W^2_{k,l}$

其中，$N$是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数$\lambda$来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。

除了上述理由外，引入正则化惩罚还带来很多良好的性质，这些性质大多会在后续章节介绍。比如引入了$L2$惩罚后，SVM们就有了**最大边界（max margin）**这一良好性质。（如果感兴趣，可以查看[CS229课程](http://cs229.stanford.edu/notes/cs229-notes3.pdf)）。

其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。举个例子，假设输入向量$x=[1,1,1,1]$，两个权重向量$w_1=[1,0,0,0]$，$w_2=[0.25,0.25,0.25,0.25]$。那么$w^T_1x=w^T_2=1$，两个权重向量都得到同样的内积，但是$w_1$的$L2$惩罚是$1.0$，而$w_2$的L2惩罚是$0.25$。因此，根据$L2$惩罚来看，$w_2$更好，因为它的正则化损失更小。从直观上来看，这是因为$w_2$的权重值更小且更分散。既然$L2$惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免过拟合。

需要注意的是，和权重不同，偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此通常只对权重$W$正则化，而不正则化偏差$b$。在实际操作中，可发现这一操作的影响可忽略不计。最后，因为正则化惩罚的存在，不可能在所有的例子中得到$0$的损失值，这是因为只有当$W=0$的特殊情况下，才能得到损失值为$0$。

代码：下面是一个无正则化部分的损失函数的Python实现，有非向量化和半向量化两个形式：

```python
def L_i(x, y, W):
  """
  unvectorized version. Compute the multiclass svm loss for a single example (x,y)
  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)
    with an appended bias dimension in the 3073-rd position (i.e. bias trick)
  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)
  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)
  """
  delta = 1.0 # see notes about delta later in this section
  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class
  correct_class_score = scores[y]
  D = W.shape[0] # number of classes, e.g. 10
  loss_i = 0.0
  for j in xrange(D): # iterate over all wrong classes
    if j == y:
      # skip for the true class to only loop over incorrect classes
      continue
    # accumulate loss for the i-th example
    loss_i += max(0, scores[j] - correct_class_score + delta)
  return loss_i

def L_i_vectorized(x, y, W):
  """
  A faster half-vectorized implementation. half-vectorized
  refers to the fact that for a single example the implementation contains
  no for loops, but there is still one loop over the examples (outside this function)
  """
  delta = 1.0
  scores = W.dot(x)
  # compute the margins for all classes in one vector operation
  margins = np.maximum(0, scores - scores[y] + delta)
  # on y-th position scores[y] - scores[y] canceled and gave delta. We want
  # to ignore the y-th position and only consider margin on max wrong class
  margins[y] = 0
  loss_i = np.sum(margins)
  return loss_i

def L(X, y, W):
  """
  fully-vectorized implementation :
  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)
  - y is array of integers specifying correct class (e.g. 50,000-D array)
  - W are weights (e.g. 10 x 3073)
  """
  # evaluate loss over all examples in X without using any for loops
  # left as exercise to reader in the assignment
```

在本小节的学习中，一定要记得SVM损失采取了一种特殊的方法，使得能够衡量对于训练数据预测分类和实际分类标签的一致性。还有，对训练集中数据做出准确分类预测和让损失值最小化这两件事是等价的。

> 接下来要做的，就是找到能够使损失值最小化的权重了。

##### 实际考虑

**设置Delta**：你可能注意到上面的内容对超参数$\Delta$及其设置是一笔带过，那么它应该被设置成什么值？需要通过交叉验证来求得吗？现在看来，该超参数在绝大多数情况下设为$\Delta=1.0$都是安全的。超参数$\Delta$和$\lambda$看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。理解这一点的关键是要知道，权重$W$的大小对于分类分值有直接影响（当然对他们的差异也有直接影响）：当我们将$W$中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如$\Delta=1$或$\Delta=100$）从某些角度来看是没意义的，因为权重自己就可以控制差异变大和缩小。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过正则化强度$\lambda$来控制）。

**与二元支持向量机（Binary Support Vector Machine）的关系**：在学习本课程前，你可能对于二元支持向量机有些经验，它对于第i个数据的损失计算公式是：

$\displaystyle L_i=Cmax(0,1-y_iw^Tx_i)+R(W)$

其中，$C$是一个超参数，并且$y_i\in\{-1,1\}$。可以认为本章节介绍的SVM公式包含了上述公式，上述公式是多类支持向量机公式只有两个分类类别的特例。也就是说，如果我们要分类的类别只有两个，那么公式就化为二元SVM公式。这个公式中的$C$和多类SVM公式中的都控制着同样的权衡，而且它们之间的关系是$C\propto\frac{1}{\lambda}$

**备注**：**在初始形式中进行最优化**。如果在本课程之前学习过SVM，那么对kernels，duals，SMO算法等将有所耳闻。在本课程（主要是神经网络相关）中，损失函数的最优化的始终在非限制初始形式下进行。很多这些损失函数从技术上来说是不可微的（比如当$x=y$时，函数$\max(x,y)$就不可微分），但是在实际操作中并不存在问题，因为通常可以使用次梯度。

**备注**：**其他多类SVM公式**。需要指出的是，本课中展示的多类SVM只是多种SVM公式中的一种。另一种常用的公式是One-Vs-All（OVA）SVM，它针对每个类和其他类训练一个独立的二元分类器。还有另一种更少用的叫做All-Vs-All（AVA）策略。我们的公式是按照[Weston and Watkins 1999 (pdf)](http://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf)版本，比OVA性能更强（在构建有一个多类数据集的情况下，这个版本可以在损失值上取到$0$，而OVA就不行。感兴趣的话在论文中查阅细节）。最后一个需要知道的公式是Structured SVM，它将正确分类的分类分值和非正确分类中的最高分值的边界最大化。理解这些公式的差异超出了本课程的范围。本课程笔记介绍的版本可以在实践中安全使用，而被论证为最简单的OVA策略在实践中看起来也能工作的同样出色（在 Rikin等人2004年的论文[In Defense of One-Vs-All Classification (pdf)](http://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf)中可查）。

#### Softmax分类器

SVM是最常用的两个分类器之一，而另一个就是**Softmax分类器**，它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出$f(x_i,W)$作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射$f(x_i,W)=Wx_i$保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将折叶损失（hinge loss）替换为交叉熵损失（cross-entropy loss）。公式如下： 

$\displaystyle Li=-log(\frac{e^{f_{y_i}}}{\displaystyle \sum_je^{f_j}})$或等价的$\displaystyle L_i=-f_{y_i}+log(\sum_je^{f_j})$

在上式中，使用$f_j$来表示分类评分向量$f$中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值$L_i$的均值与正则化损失$R(W)$之和。其中函数$\displaystyle f_j(z)=\frac{e^{z_j}}{\displaystyle \sum_ke^{z_k}}$被称作softmax 函数：其输入值是一个向量，向量中元素为任意实数的评分值（$z$中的），函数对其进行压缩，输出一个向量，其中每个元素值在$0$到$1$之间，且所有元素之和为$1$。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。

**信息理论视角**：在“真实”分布和估计分布之间的交叉熵定义如下：

$\displaystyle H(p,q)=-\sum_xp(x) \log q(x)$

因此，Softmax分类器所做的就是最小化在估计分类概率（就是上面的$\displaystyle \frac{e^{f_{y_i}}}{\displaystyle \sum_je^{f_j}}$）和“真实”分布之间的交叉熵，在这个解释中，“真实”分布就是所有概率密度都分布在正确的类别上（比如：$p=[0,...1,...,0]$中在的位置就有一个单独的$1$）。还有，既然交叉熵可以写成熵和**相对熵（Kullback-Leibler divergence）**$H(p,q)=H(p)+D_{KL}(p||q)$，并且delta函数$p$的熵是$0$，那么就能等价的看做是对两个分布之间的相对熵做最小化操作。换句话说，交叉熵损失函数“想要”预测分布的所有**概率密度**都在正确分类上。

**译者注**：Kullback-Leibler差异（Kullback-Leibler Divergence）也叫做相对熵（Relative Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。

**概率论解释**：先看下面的公式：

$P(y_i|x_i,W)=\frac{e^{f_{y_i}}}{\sum_je^{f_j}}$

可以解释为是给定图像数据$x_i$，以$W$为参数，分配给正确分类标签$y_i$的归一化概率。为了理解这点，请回忆一下Softmax分类器将输出向量$f$中的评分值解释为没有归一化的**对数概率**。那么以这些数值做指数函数的幂就得到了没有归一化的概率，而除法操作则对数据进行了归一化处理，使得这些概率的和为1。从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行**最大似然估计（MLE）**。该解释的另一个好处是，损失函数中的正则化部分$R(W)$可以被看做是权重矩阵$W$的高斯先验，这里进行的是**最大后验估计（MAP）**而不是最大似然估计。提及这些解释只是为了让读者形成直观的印象，具体细节就超过本课程范围了。

**实操事项：数值稳定**。编程实现softmax函数计算的时候，中间项$e^{f_{y_i}}$和$\displaystyle \sum_j e^{f_j}$因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数$C$，并把它变换到求和之中，就能得到一个从数学上等价的公式：

$\displaystyle \frac{e^{f_{y_i}}}{\displaystyle \sum_je^{f_j}}=\frac{Ce^{f_{y_i}}}{\displaystyle C\sum_je^{f_j}}=\frac{e^{f_{y_i}+\log{C}}}{\displaystyle \sum_je^{f_j+\log{C}}}$

$C$的值可自由选择，不会影响计算结果，通过使用这个技巧可以提高计算中的数值稳定性。通常将$C$设为$\log{C}=-max_jf_j$。该技巧简单地说，就是应该将向量中的数值进行平移，使得最大值为$0$。代码实现如下：

```python
f = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大
p = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸

# 那么将f中的值平移到最大值为0：
f -= np.max(f) # f becomes [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果
```

**让人迷惑的命名规则**：精确地说，SVM分类器使用的是**折叶损失（hinge loss）**，有时候又被称为**最大边界损失（max-margin loss）**。Softmax分类器使用的是**交叉熵损失（corss-entropy loss）**。Softmax分类器的命名是从softmax函数那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为$1$，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。

#### SVM和Softmax的比较

下图有助于区分这 Softmax和SVM这两种分类器：

————————————————————————————————————————

![img](https://pic1.zhimg.com/a90ce9e0ff533f3efee4747305382064_b.png)

针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量$f$（本节中是通过矩阵乘来实现）。不同之处在于对f中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别$2$）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的**对数概率**，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是$1.58$，Softmax的最终的损失值是$0.452$，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。

————————————————————————————————————————

**Softmax分类器为每个分类提供了“可能性”**：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个$[12.5, 0.6, -23.0]$对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是$[0.9, 0.09, 0.01]$，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数$λ$直接决定的，$λ$是你能直接控制的一个输入参数。举个例子，假设$3$个分类的原始分数是$[1, -2, 0]$，那么softmax函数就会计算：

$ [1,-2,0]\to[e^1,e^{-2},e^0]=[2.71,0.14,1]\to[0.7,0.04,0.26]$

现在，如果正则化参数$λ$更大，那么权重$W$就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧$[0.5, -1, 0]$，那么softmax函数的计算就是：

$[0.5,-1,0]\to[e^{0.5},e^{-1},e^0]=[1.65,0.73,1]\to[0.55,0.12,0.33]$

现在看起来，概率的分布就更加分散了。还有，随着正则化参数$λ$不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释。

**在实际使用中，SVM和Softmax经常是相似的**：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“**局部目标化（local objective）**”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是$[10, -2, 3]$的数据，其中第一个分类是正确的。那么一个SVM（$\Delta=1$）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是$0$。SVM对于数字个体的细节是不关心的：如果分数是$[10, -100, -100]$或者$[10, 9, 9]$，对于SVM来说没设么不同，只要满足超过边界值等于$1$，那么损失值就等于$0$。

对于softmax分类器，情况则不同。对于$[10, 9, 9]$来说，计算出的损失值就远远高于$[10, -100, -100]$的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。

### 交互式的网页Demo

————————————————————————————————————————

![img](https://pic1.zhimg.com/a68bbfd4465689c6d65b3eae9c24c934_r.jpg)

我们实现了一个交互式的网页原型，来帮助读者直观地理解线性分类器。原型将损失函数进行可视化，画面表现的是对于$2$维数据的$3$种类别的分类。原型在课程进度上稍微超前，展现了最优化的内容，最优化将在下一节课讨论。

————————————————————————————————————————

### 小结

总结如下：

- 定义了从图像像素映射到不同类别的分类评分的评分函数。在本节中，评分函数是一个基于权重$W$和偏差$b$的线性函数。
- 与kNN分类器不同，**参数方法**的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重$W$进行一个矩阵乘法运算。
- 介绍了偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵。
- 定义了损失函数（介绍了SVM和Softmax线性分类器最常用的2个损失函数）。损失函数能够衡量给出的参数集与训练集数据真实类别情况之间的一致性。在损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的。

现在我们知道了如何基于参数，将数据集中的图像映射成为分类的评分，也知道了两种不同的损失函数，它们都能用来衡量算法分类预测的质量。但是，如何高效地得到能够使损失值最小的参数呢？这个求得最优参数的过程被称为最优化，将在下节课中进行介绍。

### 拓展阅读

下面的内容读者可根据兴趣选择性阅读。

- [Deep Learning using Linear Support Vector Machines](http://arxiv.org/abs/1306.0239)一文的作者是Tang Charlie，论文写于2013年，展示了一些L2SVM比Softmax表现更出色的结果。

---

## 原文：[[optimization notes\]](https://cs231n.github.io/optimization-1)。

翻译：最优化笔记[（上）](https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit)[（下）](https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit)。

> 该笔记介绍了图像分类任务的第三个关键部分：最优化。内容列表如下：
>
> - 简介
>
> - 损失函数可视化
>
> - 最优化
>
> - - 策略#1：随机搜索
>   - 策略#2：随机局部搜索
>   - 策略#3：跟随梯度 *译者注：上篇截止处*
>
> - 梯度计算
>
> - - 使用有限差值进行数值计算
>   - 微分计算梯度
>
> - 梯度下降
>
> - 小结

### 简介

在上一节中，我们介绍了图像分类任务中的两个关键部分：

1. 基于参数的**评分函数**。该函数将原始图像像素映射为分类评分值（例如：一个线性函数）。
2. **损失函数**。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。

上节中，线性函数的形式是$f(x_i,W)=Wx_i$，而SVM实现的公式是：

$$L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + 1) \right] + \alpha R(W)$$

对于图像数据$x_i$，如果基于参数集$W$做出的分类预测与真实情况比较一致，那么计算出来的损失值$L$就很低。现在介绍第三个，也是最后一个关键部分：**最优化Optimization**。最优化是寻找能使得损失函数值最小化的参数$W$的过程。

**铺垫**：一旦理解了这三个部分是如何相互运作的，我们将会回到第一个部分（基于参数的函数映射），然后将其拓展为一个远比线性函数复杂的函数：首先是神经网络，然后是卷积神经网络。而损失函数和最优化过程这两个部分将会保持相对稳定。

### 损失函数可视化

本课中讨论的损失函数一般都是定义在高维度的空间中（比如，在CIFAR-10中一个线性分类器的权重矩阵大小是$[10 * 3073]$，就有$30730$个参数），这样要将其可视化就很困难。然而办法还是有的，在$1$个维度或者$2$个维度的方向上对高维空间进行切片，就能得到一些直观感受。例如，随机生成一个权重矩阵$W$，该矩阵就与高维空间中的一个点对应。然后沿着某个维度方向前进的同时记录损失函数值的变化。换句话说，就是生成一个随机的方向$W_1$并且沿着此方向计算损失值，计算方法是根据不同的$a$值来计算$L(W+aW_1)$。这个过程将生成一个图表，其x轴是$a$值，y轴是损失函数值。同样的方法还可以用在两个维度上，通过改变$a,b$来计算损失值$L(W+aW_1+bW_2)$，从而给出二维的图像。在图像中，$a,b$可以分别用x和y轴表示，而损失函数的值可以用颜色变化表示：

————————————————————————————————————————

 ![img](https://pic2.zhimg.com/94dd0714f65ef94b3cbfff4780b1988d_b.png)

一个无正则化的多类SVM的损失函数的图示。左边和中间只有一个样本数据，右边是CIFAR-10中的$100$个数据。**左**：$a$值变化在某个维度方向上对应的的损失值变化。**中和右**：两个维度方向上的损失值切片图，蓝色部分是低损失值区域，红色部分是高损失值区域。注意损失函数的分段线性结构。多个样本的损失值是总体的平均值，所以右边的碗状结构是很多的分段线性结构的平均（比如中间这个就是其中之一）。

—————————————————————————————————————————

我们可以通过数学公式来解释损失函数的分段线性结构。对于一个单独的数据，有损失函数的计算公式如下：

$$L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \right]$$

通过公式可见，每个样本的数据损失值是以$W$为参数的线性函数的总和（零阈值来源于$\max(0,-)$函数）。$W$的每一行（即$w_j$），有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。为进一步阐明，假设有一个简单的数据集，其中包含有$3$个只有$1$个维度的点，数据集数据点有$3$个类别。那么完整的无正则化SVM的损失值计算如下：
$$
\begin{align}
L_0 = & \max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\
L_1 = & \max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\
L_2 = & \max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\
L = & (L_0 + L_1 + L_2)/3
\end{align}
$$
因为这些例子都是一维的，所以数据$x_i$和权重$w_j$都是数字。观察$w_0$，可以看到上面的式子中一些项是$w_0$的线性函数，且每一项都会与$0$比较，取两者的最大值。可作图如下：

——————————————————————————————————————

![img](https://pic3.zhimg.com/3f6fbcd487b1c214e8fea1ea66eb413e_b.png)

从一个维度方向上对数据损失值的展示。$x$轴方向就是一个权重，$y$轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与$0$阈值的比较。完整的SVM数据损失就是这个形状的$30730$维版本。——————————————————————————————————————

需要多说一句的是，你可能根据SVM的损失函数的碗状外观猜出它是一个[凸函数](https://en.wikipedia.org/wiki/Convex_function)。关于如何高效地最小化凸函数的论文有很多，你也可以学习斯坦福大学关于（[凸函数最优化](http://stanford.edu/~boyd/cvxbook/)）的课程。但是一旦我们将函数扩展到神经网络，目标函数就就不再是凸函数了，图像也不会像上面那样是个碗状，而是凹凸不平的复杂地形形状。

不可导的损失函数。作为一个技术笔记，你要注意到：由于$max$操作，损失函数中存在一些不可导点（kinks），这些点使得损失函数不可微，因为在这些不可导点，梯度是没有定义的。但是[次梯度（subgradient）](https://en.wikipedia.org/wiki/Subderivative)依然存在且常常被使用。在本课中，我们将交换使用次梯度和梯度两个术语。

### 最优化 Optimization

重申一下：损失函数可以量化某个具体权重集$W$的质量。而最优化的目标就是找到能够最小化损失函数值的$W$ 。我们现在就朝着这个目标前进，实现一个能够最优化损失函数的方法。对于有一些经验的同学，这节课看起来有点奇怪，因为使用的例子（SVM 损失函数）是一个凸函数问题。但是要记得，最终的目标是不仅仅对凸函数做最优化，而是能够最优化一个神经网络，而对于神经网络是不能简单的使用凸函数的最优化技巧的。

#### 策略#1：一个差劲的初始方案：随机搜索

既然确认参数集**$W$**的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以随机尝试很多不同的权重，然后看其中哪个最好。过程如下：

```python
# 假设X_train的每一列都是一个数据样本（比如3073 x 50000）
# 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）
# 假设函数L对损失函数进行评价

bestloss = float("inf") # Python assigns the highest possible float value
for num in xrange(1000):
  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters
  loss = L(X_train, Y_train, W) # get the loss over the entire training set
  if loss < bestloss: # keep track of the best solution
    bestloss = loss
    bestW = W
  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)

# 输出:
# in attempt 0 the loss was 9.401632, best 9.401632
# in attempt 1 the loss was 8.959668, best 8.959668
# in attempt 2 the loss was 9.044034, best 8.959668
# in attempt 3 the loss was 9.278948, best 8.959668
# in attempt 4 the loss was 8.857370, best 8.857370
# in attempt 5 the loss was 8.943151, best 8.857370
# in attempt 6 the loss was 8.605604, best 8.605604
# ... (trunctated: continues for 1000 lines)
```

在上面的代码中，我们尝试了若干随机生成的权重矩阵$W$，其中某些的损失值较小，而另一些的损失值大些。我们可以把这次随机搜索中找到的最好的权重$W$取出，然后去跑测试集：

```python
# 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]
scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples
# 找到在每列中评分值最大的索引（即预测的分类）
Yte_predict = np.argmax(scores, axis = 0)
# 以及计算准确率
np.mean(Yte_predict == Yte)
# 返回 0.1555
```

验证集上表现最好的权重$W$跑测试集的准确率是**15.5%，**而完全随机猜的准确率是`10%`，如此看来，这个准确率对于这样一个不经过大脑的策略来说，还算不错嘛！

**核心思路**：**迭代优化**。当然，我们肯定能做得更好些。核心思路是：虽然找到最优的权重$W$非常困难，甚至是不可能的（尤其当$W$中存的是整个神经网络的权重的时候），但如果问题转化为：对一个权重矩阵集$W$取优，使其损失值稍微减少。那么问题的难度就大大降低了。换句话说，我们的方法从一个随机的$W$开始，然后对其迭代取优，每次都让它的损失值变得更小一点。

> 我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。

**蒙眼徒步者的比喻**：一个助于理解的比喻是把你自己想象成一个蒙着眼睛的徒步者，正走在山地地形上，目标是要慢慢走到山底。在CIFAR-10的例子中，这山是$30730$维的（因为$W$是$3073 * 10$）。我们在山上踩的每一点都对应一个的损失值，该损失值可以看做该点的海拔高度。

#### 策略#2：随机本地搜索

第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机$W$开始，然后生成一个随机的扰动$\delta W$，只有当$W+\delta W$的损失值变低，我们才会更新。这个过程的具体代码如下：

```python
W = np.random.randn(10, 3073) * 0.001 # 生成随机初始W
bestloss = float("inf")
for i in xrange(1000):
  step_size = 0.0001
  Wtry = W + np.random.randn(10, 3073) * step_size
  loss = L(Xtr_cols, Ytr, Wtry)
  if loss < bestloss:
    W = Wtry
    bestloss = loss
  print 'iter %d loss is %f' % (i, bestloss)
```

使用同样的数据（$1000$），这个方法可以得到**21.4%**的分类准确率。这个比策略一好，但是依然过于浪费计算资源。

#### 策略#3：跟随梯度

前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。这个方向就是损失函数的梯度（gradient）。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。

在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数derivatives）。对一维函数的求导公式如下：
$$
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
$$
当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。

### 梯度计算

计算梯度有两种方法：一个是缓慢的近似方法（**数值梯度法**），但实现相对简单。另一个方法（**分析梯度法**）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。现在对两种方法进行介绍：

#### 利用有限差值计算梯度

上节中的公式已经给出数值计算梯度的方法。下面代码是一个输入为函数$f$和向量$x$，计算$f$的梯度的通用函数，它返回函数$f$在点$x$处的梯度：

```python
def eval_numerical_gradient(f, x):
  """  
  一个f在x处的数值梯度法的简单实现
  - f是只有一个参数的函数
  - x是计算梯度的点
  """ 

  fx = f(x) # 在原点计算函数值
  grad = np.zeros(x.shape)
  h = 0.00001

  # 对x中所有的索引进行迭代
  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
  while not it.finished:

    # 计算x+h处的函数值
    ix = it.multi_index
    old_value = x[ix]
    x[ix] = old_value + h # 增加h
    fxh = f(x) # 计算f(x + h)
    x[ix] = old_value # 存到前一个值中 (非常重要)

    # 计算偏导数
    grad[ix] = (fxh - fx) / h # 坡度
    it.iternext() # 到下个维度

  return grad
```

根据上面的梯度公式，代码对所有维度进行迭代，在每个维度上产生一个很小的变化$h$，通过观察函数值变化，计算函数在该维度上的偏导数。最后，所有的梯度存储在变量**$grad$**中。

实践考量：注意在数学公式中，$h$的取值是趋近于$0$的，然而在实际中，用一个很小的数值（比如例子中的$1e-5$）就足够了。在不产生数值计算出错的理想前提下，你会使用尽可能小的$h$。还有，实际中用**中心差值公式（centered difference formula）**效果较好。细节可查看[wiki](http://en.wikipedia.org/wiki/Numerical_differentiation)。

可以使用上面这个公式来计算任意函数在任意点上的梯度。下面计算权重空间中的某些随机点上，CIFAR-10损失函数的梯度：

```python
# 要使用上面的代码我们需要一个只有一个参数的函数
# (在这里参数就是权重)所以也包含了X_train和Y_train
def CIFAR10_loss_fun(W):
  return L(X_train, Y_train, W)

W = np.random.rand(10, 3073) * 0.001 # 随机权重向量
df = eval_numerical_gradient(CIFAR10_loss_fun, W) # 得到梯度
```

梯度告诉我们损失函数在每个维度上的斜率，以此来进行更新：

```python
loss_original = CIFAR10_loss_fun(W) # 初始损失值
print 'original loss: %f' % (loss_original, )

# 查看不同步长的效果
for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:
  step_size = 10 ** step_size_log
  W_new = W - step_size * df # 权重空间中的新位置
  loss_new = CIFAR10_loss_fun(W_new)
  print 'for step size %f new loss: %f' % (step_size, loss_new)

# 输出:
# original loss: 2.200718
# for step size 1.000000e-10 new loss: 2.200652
# for step size 1.000000e-09 new loss: 2.200057
# for step size 1.000000e-08 new loss: 2.194116
# for step size 1.000000e-07 new loss: 2.135493
# for step size 1.000000e-06 new loss: 1.647802
# for step size 1.000000e-05 new loss: 2.844355
# for step size 1.000000e-04 new loss: 25.558142
# for step size 1.000000e-03 new loss: 254.086573
# for step size 1.000000e-02 new loss: 2539.370888
# for step size 1.000000e-01 new loss: 25392.214036
```

**在梯度负方向上更新**：在上面的代码中，为了计算**W_new**，要注意我们是向着梯度$df$的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。

**步长的影响**：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。在后续的课程中可以看到，选择步长（也叫作学习率）将会是神经网络训练中最重要（也是最头痛）的超参数设定之一。还是用蒙眼徒步者下山的比喻，这就好比我们可以感觉到脚朝向的不同方向上，地形的倾斜程度不同。但是该跨出多长的步长呢？不确定。如果谨慎地小步走，情况可能比较稳定但是进展较慢（这就是步长较小的情况）。相反，如果想尽快下山，那就大步走吧，但结果也不一定尽如人意。在上面的代码中就能看见反例，在某些点如果步长过大，反而可能越过最低点导致更高的损失值。

————————————————————————————————————————

![img](https://pic1.zhimg.com/d8b52b9b9ca31e2132c436c39af2943c_b.jpg)

将步长效果视觉化的图例。从某个具体的点$W$开始计算梯度（白箭头方向是负梯度方向），梯度告诉了我们损失函数下降最陡峭的方向。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为**学习率**）将会是我们在调参中最重要的超参数之一。————————————————————————————————————————

**效率问题**：你可能已经注意到，计算数值梯度的复杂性和参数的量线性相关。在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度。现代神经网络很容易就有上千万的参数，因此这个问题只会越发严峻。显然这个策略不适合大规模数据，我们需要更好的策略。

#### 微分分析计算梯度

使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为我们对于$h$值是选取了一个很小的数值，但真正的梯度定义中$h$趋向$0$的极限），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做梯度检查。

用SVM的损失函数在某个数据点上的计算来举例：

$$L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]$$

可以对函数进行微分。比如，对$w_{y_i}$进行微分得到：

$$\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta > 0) \right) x_i$$

译者注：原公式中$1$为空心字体，尝试`\mathbb{}`等多种方法仍无法实现，请知友指点。

其中$1$是一个示性函数，如果括号中的条件为真，那么函数值为$1$，如果为假，则函数值为$0$。虽然上述公式看起来复杂，但在代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以$x_i$就是梯度了。注意，这个梯度只是对应正确分类的$W$的行向量的梯度，那些$i\not=y_i$行的梯度是：

$$\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta > 0) x_i$$

一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了。

### 梯度下降

现在可以计算损失函数的梯度了，程序重复地计算梯度然后对参数进行更新，这一过程称为*梯度下降*，他的**普通**版本是这样的：

```python
# 普通的梯度下降

while True:
  weights_grad = evaluate_gradient(loss_fun, data, weights)
  weights += - step_size * weights_grad # 进行梯度更新
```

这个简单的循环在所有的神经网络核心库中都有。虽然也有其他实现最优化的方法（比如LBFGS），但是到目前为止，梯度下降是对神经网络的损失函数最优化中最常用的方法。课程中，我们会在它的循环细节增加一些新的东西（比如更新的具体公式），但是核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。

**小批量数据梯度下降（Mini-batch gradient descent）**：在大规模的应用中（比如ILSVRC挑战赛），训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的小批量（batches）数据。例如，在目前最高水平的卷积神经网络中，一个典型的小批量包含256个例子，而整个训练集是多少呢？一百二十万个。这个小批量数据就用来实现一个参数更新：

```python
# 普通的小批量数据梯度下降

while True:
  data_batch = sample_training_data(data, 256) # 256个数据
  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
  weights += - step_size * weights_grad # 参数更新
```

这个方法之所以效果不错，是因为训练集中的数据都是相关的。要理解这一点，可以想象一个极端情况：在ILSVRC中的120万个图像是1000张不同图片的复制（每个类别1张图片，每张图片有1200张复制）。那么显然计算这1200张复制图像的梯度就应该是一样的。对比120万张图片的数据损失的均值与只计算1000张的子集的数据损失均值时，结果应该是一样的。实际情况中，数据集肯定不会包含重复图像，那么小批量数据的梯度就是对整个数据集梯度的一个近似。因此，在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新。

小批量数据策略有个极端情况，那就是每个批量中只有$1$个数据样本，这种策略被称为**随机梯度下降（Stochastic Gradient Descent 简称SGD）**，有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算$100$个数据 比$100$次计算$1$个数据要高效很多。即使SGD在技术上是指每次使用1个数据来计算梯度，你还是会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如$32，64，128$等。之所以使用$2$的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是$2$的倍数，那么运算更快。

### 小结

————————————————————————————————————————

![img](https://pic2.zhimg.com/03b3eccf18ee3760e219f9f95ec14305_b.png)

信息流的总结图例。数据集中的$(x,y)$是给定的。权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量$f$中。损失函数包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分$f$和实际标签$y$之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，我们计算权重的梯度（如果愿意的话，也可以计算数据上的梯度），然后使用它们来实现参数的更新。

—————————————————————————————————————————

在本节课中：

- 将损失函数比作了一个**高维度的最优化地形**，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。
- 提出了**迭代优化的思想**，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。
- 函数的**梯度**给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是h，用来计算数值梯度）。
- 参数更新需要有技巧地设置**步长**。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。
- 讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用**梯度检查**来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。
- 介绍了**梯度下降**算法，它在循环中迭代地计算梯度并更新参数。

**预告**：这节课的核心内容是：理解并能计算损失函数关于权重的梯度，是设计、训练和理解神经网络的核心能力。下节中，将介绍如何使用链式法则来高效地计算梯度，也就是通常所说的**反向传播（backpropagation）机制**。该机制能够对包含卷积神经网络在内的几乎所有类型的神经网络的损失函数进行高效的最优化。

---

## 原文：[[backprop notes\]](https://cs231n.github.io/optimization-2)。

翻译：[反向传播笔记](https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit)。

> 该笔记本将帮助读者**对反向传播形成直观而专业的理解**。反向传播是利用链式法则递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。内容里列表如下：
> 简介简单表达式和理解梯度复合表达式，链式法则，反向传播直观理解反向传播模块：Sigmoid例子反向传播实践：分段计算回传流中的模式用户向量化操作的梯度小结

### 简介

**目标**：本节将帮助读者对**反向传播**形成直观而专业的理解。反向传播是利用**链式法则**递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。

**问题陈述**：这节的核心问题是：给定函数$f(x)$，其中$x$是输入数据的向量，需要计算函数$f$关于$x$的梯度，也就是$\nabla f(x)$。

**目标**：之所以关注上述问题，是因为在神经网络中$f$对应的是损失函数（$L$），输入$x$里面包含训练数据和神经网络的权重。举个例子，损失函数可以是SVM的损失函数，输入则包含了训练数据$(x_i,y_i),i=1...N$、权重$W$和偏差$b$。注意训练集是给定的（在机器学习中通常都是这样），而权重是可以控制的变量。因此，即使能用反向传播计算输入数据$x_i$上的梯度，但在实践为了进行参数更新，通常也只计算参数（比如$W,b$）的梯度。然而 $x_i$的梯度有时仍然是有用的：比如将神经网络所做的事情可视化便于直观理解的时候，就能用上。

如果读者之前对于利用链式法则计算偏微分已经很熟练，仍然建议浏览本篇笔记。因为它呈现了一个相对成熟的反向传播视角，在该视角中能看见基于实数值回路的反向传播过程，而对其细节的理解和收获将帮助读者更好地通过本课程。

### 简单表达式和理解梯度

从简单表达式入手可以为复杂表达式打好符号和规则基础。先考虑一个简单的二元乘法函数$f(x,y)=xy$。对两个输入变量分别求偏导数还是很简单的：

$$f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x$$

**解释**：牢记这些导数的意义：函数变量在某个点周围的极小区域内变化，而导数就是变量变化导致的函数在该方向上的变化率。

$$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$

注意等号左边的分号和等号右边的分号不同，不是代表分数。相反，这个符号表示操作符$\frac{d}{dx}$被应用于函数$f$，并返回一个不同的函数（导数）。对于上述公式，可以认为$h$值非常小，函数可以被一条直线近似，而导数就是这条直线的斜率。换句话说，每个变量的导数指明了整个表达式对于该变量的值的敏感程度。比如，若$x=4,y=-3$，则$f(x,y)=-12$，$x$的导数$\frac{\partial f} {\partial x}=-3$。这就说明如果将变量$x$的值变大一点，整个表达式的值就会变小（原因在于负号），而且变小的量是$x$变大的量的三倍。通过重新排列公式可以看到这一点（$f(x+h)=f(x)+h\frac{df(x)}{dx}$）。同样，因为$\frac{\partial f} {\partial y}=4$，可以知道如果将$y$的值增加$h$，那么函数的输出也将增加（原因在于正号），且增加量是$4h$。

>  **函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度**。

如上所述，梯度$\nabla f$是偏导数的向量，所以有$\nabla f(x)=[\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]=[y,x]$。即使是梯度实际上是一个向量，仍然通常使用类似“*$x$上的梯度*”的术语，而不是使用如“*$x$的偏导数*”的正确说法，原因是因为前者说起来简单。我们也可以对加法操作求导：

$$f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1$$

这就是说，无论其值如何，$x,y$的导数均为$1$。这是有道理的，因为无论增加$x,y$中任一个的值，函数$f$的值都会增加，并且增加的变化率独立于$x,y$的具体值（情况和乘法操作不同）。取最大值操作也是常常使用的：

$$f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)$$

上式是说，如果该变量比另一个变量大，那么梯度是$1$，反之为$0$。例如，若$x=4,y=2$，那么$max$是$4$，所以函数对于$y$就不敏感。也就是说，在$y$上增加$h$，函数还是输出为$4$，所以梯度是$0$：因为对于函数输出是没有效果的。当然，如果给$y$增加一个很大的量，比如大于$2$，那么函数$f$的值就变化了，但是导数并没有指明输入量有巨大变化情况对于函数的效果，他们只适用于输入量变化极小时的情况，因为定义已经指明：$lim_{h\to 0}$。

### 使用链式法则计算复合表达式

现在考虑更复杂的包含多个函数的复合函数，比如$f(x,y,z)=(x+y)z$。虽然这个表达足够简单，可以直接微分，但是在此使用一种有助于读者直观理解反向传播的方法。将公式分成两部分：$q=x+y$和$f=qz$。在前面已经介绍过如何对这分开的两个公式进行计算，因为$f$是$q$和$z$相乘，所以$\displaystyle\frac{\partial f}{\partial q}=z,\frac{\partial f}{\partial z}=q$，又因为$q$是$x$加$y$，所以$\displaystyle\frac{\partial q}{\partial x}=1,\frac{\partial q}{\partial y}=1$。然而，并不需要关心中间量$q$的梯度，因为$\frac{\partial f}{\partial q}$没有用。相反，函数$f$关于$x,y,z$的梯度才是需要关注的。

**链式法则**指出将这些梯度表达式链接起来的正确方式是相乘，比如$\displaystyle\frac{\partial f}{\partial x}=\frac{\partial f}{\partial q}\frac{\partial q}{\partial x}$。在实际操作中，这只是简单地将两个梯度数值相乘，示例代码如下：

```python
# 设置输入值
x = -2; y = 5; z = -4

# 进行前向传播
q = x + y # q becomes 3
f = q * z # f becomes -12

# 进行反向传播:
# 首先回传到 f = q * z
dfdz = q # df/dz = q, 所以关于z的梯度是3
dfdq = z # df/dq = z, 所以关于q的梯度是-4
# 现在回传到q = x + y
dfdx = 1.0 * dfdq # dq/dx = 1. 这里的乘法是因为链式法则
dfdy = 1.0 * dfdq # dq/dy = 1
```

最后得到变量的梯度$[dfdx, dfdy, dfdz]$，它们告诉我们函数$f$对于变量$[x, y, z]$的敏感程度。这是一个最简单的反向传播。一般会使用一个更简洁的表达符号，这样就不用写$df$了。这就是说，用$dq$来代替$dfdq$，且总是假设梯度是关于最终输出的。

这次计算可以被可视化为如下计算线路图像：

————————————————————————————————————————

![img](https://pic4.zhimg.com/213da7f66594510b45989bd134fc2d8b_b.jpg)

上图的真实值计算线路展示了计算的视觉化过程。**前向传播**从输入计算到输出（绿色），**反向传播**从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。

————————————————————————————————————————

### 反向传播的直观理解

反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：①这个门的输出值和②其输出值关于输入值的局部梯度。门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。

> 这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等。

下面通过例子来对这一过程进行理解。加法门收到了输入$[-2, 5]$，计算输出是$3$。既然这个门是加法操作，那么对于两个输入的局部梯度都是$+1$。网络的其余部分计算出最终值为$-12$。在反向传播时将递归地使用链式法则，算到加法门（是乘法门的输入）的时候，知道加法门的输出的梯度是$-4$。如果网络如果想要输出值更高，那么可以认为它会想要加法门的输出更小一点（因为负号），而且还有一个$4$的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让$-4$乘以$x$和$y$的局部梯度，$x$和$y$的局部梯度都是$1$，所以最终都是$-4$）。可以看到得到了想要的效果：如果$x，y$减小（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。

因此，反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的输出值更高。

### 模块化：Sigmoid例子

上面介绍的门是相对随意的。任何可微分的函数都可以看做门。可以将多个门组合成一个门，也可以根据需要将一个函数分拆成多个门。现在看看一个表达式：

$$f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$

在后面的课程中可以看到，这个表达式描述了一个含输入$x$和权重$w$的2维的神经元，该神经元使用了**sigmoid激活函数**。但是现在只是看做是一个简单的输入为$x$和$w$，输出为一个数字的函数。这个函数是由多个门组成的。除了上文介绍的加法门，乘法门，取最大值门，还有下面这4种：
$$
f(x) = \frac{1}{x} \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = -1/x^2 \\
f_c(x) = c + x \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = 1 \\
f(x) = e^x \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = e^x \\
f_a(x) = ax \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = a
$$
其中，函数$f_c$使用对输入值进行了常量$c$的平移，$f_a$将输入值扩大了常量$a$倍。它们是加法和乘法的特例，但是这里将其看做一元门单元，因为确实需要计算常量$c,a$的梯度。整个计算线路如下：

———————————————————————————————————————

![-img](https://pic1.zhimg.com/0799b3d6e5e92245ee937db3c26d1b80_b.png)

使用$sigmoid$激活函数的$2$维神经元的例子。输入是$[x_0, x_1]$，可学习的权重是$[w_0, w_1, w_2]$。一会儿会看见，这个神经元对输入数据做点积运算，然后其激活数据被$sigmoid$函数挤压到$0$到$1$之间。

————————————————————————————————————————

在上面的例子中可以看见一个函数操作的长链条，链条上的门都对$w$和$x$的点积结果进行操作。该函数被称为$sigmoid$函数$\sigma (x)$。$sigmoid$函数关于其输入的求导是可以简化的（使用了在分子上先加后减$1$的技巧）：
$$
\sigma(x) = \frac{1}{1+e^{-x}} \rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) = \left( 1 - \sigma(x) \right) \sigma(x)
$$
可以看到梯度计算简单了很多。举个例子，$sigmoid$表达式输入为$1.0$，则在前向传播中计算出输出为$0.73$。根据上面的公式，局部梯度为$(1-0.73) * 0.73 \approx 0.2$，和之前的计算流程比起来，现在的计算使用一个单独的简单表达式即可。因此，在实际的应用中将这些操作装进一个单独的门单元中将会非常有用。该神经元反向传播的代码实现如下：

```python
w = [2,-3,-3] # 假设一些随机数据和权重
x = [-1, -2]

# 前向传播
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid函数

# 对神经元反向传播
ddot = (1 - f) * f # 点积变量的梯度, 使用sigmoid函数求导
dx = [w[0] * ddot, w[1] * ddot] # 回传到x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # 回传到w
# 完成！得到输入的梯度
```

**实现提示**：**分段反向传播**。上面的代码展示了在实际操作中，为了使反向传播过程更加简洁，把向前传播分成不同的阶段将是很有帮助的。比如我们创建了一个中间变量$dot$，它装着w和x的点乘结果。在反向传播的时，就可以（反向地）计算出装着$w$和$x$等的梯度的对应的变量（比如$ddot$，$dx$和$dw$）。

本节的要点就是展示反向传播的细节过程，以及前向传播过程中，哪些函数可以被组合成门，从而可以进行简化。知道表达式中哪部分的局部梯度计算比较简洁非常有用，这样他们可以“链”在一起，让代码量更少，效率更高。

### 反向传播实践：分段计算

看另一个例子。假设有如下函数：

$$f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}$$

首先要说的是，这个函数完全没用，读者是不会用到它来进行梯度计算的，这里只是用来作为实践反向传播的一个例子，需要强调的是，如果对$x$或$y$进行微分运算，运算结束后会得到一个巨大而复杂的表达式。然而做如此复杂的运算实际上并无必要，因为我们不需要一个明确的函数来计算梯度，只需知道如何使用反向传播计算梯度即可。下面是构建前向传播的代码模式：

```python
x = 3 # 例子数值
y = -4

# 前向传播
sigy = 1.0 / (1 + math.exp(-y)) # 分子中的sigmoi          #(1)
num = x + sigy # 分子                                    #(2)
sigx = 1.0 / (1 + math.exp(-x)) # 分母中的sigmoid         #(3)
xpy = x + y                                              #(4)
xpysqr = xpy**2                                          #(5)
den = sigx + xpysqr # 分母                                #(6)
invden = 1.0 / den                                       #(7)
f = num * invden # 搞定！                                 #(8)
```

到了表达式的最后，就完成了前向传播。注意在构建代码$s$时创建了多个中间变量，每个都是比较简单的表达式，它们计算局部梯度的方法是已知的。这样计算反向传播就简单了：我们对前向传播时产生每个变量($sigy, num, sigx, xpy, xpysqr, den, invden$)进行回传。我们会有同样数量的变量，但是都以$d$开头，用来存储对应变量的梯度。注意在反向传播的每一小块中都将包含了表达式的局部梯度，然后根据使用链式法则乘以上游梯度。对于每行代码，我们将指明其对应的是前向传播的哪部分。

```python
# 回传 f = num * invden
dnum = invden # 分子的梯度                                         #(8)
dinvden = num                                                     #(8)
# 回传 invden = 1.0 / den 
dden = (-1.0 / (den**2)) * dinvden                                #(7)
# 回传 den = sigx + xpysqr
dsigx = (1) * dden                                                #(6)
dxpysqr = (1) * dden                                              #(6)
# 回传 xpysqr = xpy**2
dxpy = (2 * xpy) * dxpysqr                                        #(5)
# 回传 xpy = x + y
dx = (1) * dxpy                                                   #(4)
dy = (1) * dxpy                                                   #(4)
# 回传 sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)
# 回传 num = x + sigy
dx += (1) * dnum                                                  #(2)
dsigy = (1) * dnum                                                #(2)
# 回传 sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) * sigy) * dsigy                                 #(1)
# 完成! 嗷~~
```

需要注意的一些东西：

- **对前向传播变量进行缓存**：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。
- **在不同分支的梯度要相加**：如果变量$x，y$在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用$+=$而不是$=$来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。

### 回传流中的模式

一个有趣的现象是在多数情况下，反向传播中的梯度可以被很直观地解释。例如神经网络中最常用的加法、乘法和取最大值这三个门单元，它们在反向传播过程中的行为都有非常简单的解释。先看下面这个例子：

——————————————————————————————————————————

 ![img](https://pic2.zhimg.com/39162d0c528144362cc09f1965d710d1_r.jpg)

一个展示反向传播的例子。加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。

——————————————————————————————————————————

从上例可知：

- **加法门单元**把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的$+1$，所以所有输入的梯度实际上就等于输出的梯度，因为乘以$1.0$保持不变。上例中，加法门把梯度$2.00$不变且相等地路由给了两个输入。
- **取最大值门单元**对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是$1.0$，其余的是$0$。上例中，取最大值门将梯度$2.00$转给了$z$变量，因为$z$的值比$w$高，于是$w$的梯度保持为$0$。
- **乘法门单元**相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，$x$的梯度是$-4.00 * 2.00=-8.00$。

*非直观影响及其结果*。注意一种比较特殊的情况，如果乘法门单元的其中一个输入非常小，而另一个输入非常大，那么乘法门的操作将会不是那么直观：它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。在线性分类器中，权重和输入是进行点积$w^Tx_i$，这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本$x_i$乘以$1000$，那么权重的梯度将会增大$1000$倍，这样就必须降低学习率来弥补。这就是为什么数据预处理关系重大，它即使只是有微小变化，也会产生巨大影响。对于梯度在计算线路中是如何流动的有一个直观的理解，可以帮助读者调试网络。

### 用向量化操作计算梯度

上述内容考虑的都是单个变量情况，但是所有概念都适用于矩阵和向量操作。然而，在操作的时候要注意关注维度和转置操作。

**矩阵相乘的梯度**：可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作：

```python
# 前向传播
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)

# 假设我们得到了D的梯度
dD = np.random.randn(*D.shape) # 和D一样的尺寸
dW = dD.dot(X.T) #.T就是对矩阵进行转置
dX = W.T.dot(dD)
```

**提示：要分析维度**！注意不需要去记忆$dW$和$dX$的表达，因为它们很容易通过维度推导出来。例如，权重的梯度$dW$的尺寸肯定和权重矩阵$W$的尺寸是一样的，而这又是由$X$和$dD$的矩阵乘法决定的（在上面的例子中$X$和$W$都是数字不是矩阵）。总有一个方式是能够让维度之间能够对的上的。例如，$X$的尺寸是$[10 * 3]$，$dD$的尺寸是$[5 * 3]$，如果你想要$dW$和$W$的尺寸是$[5 * 10]$，那就要$dD.dot(X.T)$。

**使用小而具体的例子**：有些读者可能觉得向量化操作的梯度计算比较困难，建议是写出一个很小很明确的向量化例子，在纸上演算梯度，然后对其一般化，得到一个高效的向量化操作形式。

### 小结

- 对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。
- 讨论了**分段计算**在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。

在下节课中，将会开始定义神经网络，而反向传播使我们能高效计算神经网络各个节点关于损失函数的梯度。换句话说，我们现在已经准备好训练神经网络了，本课程最困难的部分已经过去了！ConvNets相比只是向前走了一小步。

### 参考文献

[Automatic differentiation in machine learning: a survey](http://arxiv.org/abs/1502.05767)



---

## 原文：[Neural Nets notes 1](https://cs231n.github.io/neural-networks-1/)。

翻译：神经网络笔记1[（上）](https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit)[（下）](https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit)。

> 该笔记介绍了神经网络的建模与结构，内容列表如下：
>
> 
>
> - 不用大脑做类比的快速简介
> - 单个神经元建模
>   - 生物动机和连接
>   - 作为线性分类器的单个神经元
>   - 常用的激活函数
> - 神经网络结构
>   - 层组织
>   - 前向传播计算例子
>   - 表达能力
>   - 设置层的数量和尺寸
> - 小节
> - 参考文献





---

## 原文：[Neural Nets notes 2](https://cs231n.github.io/neural-networks-2/)。

翻译：[神经网络笔记2](https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit)。

> 该笔记介绍了数据的预处理，正则化和损失函数，内容列表如下：
>
> 
>
> - 设置数据和模型
>   - 数据预处理
>   - 权重初始化
>   - 批量归一化（Batch Normalization）
>   - 正则化（L2/L1/Maxnorm/Dropout）
> - 损失函数
> - 小结







---

## 原文：[Neural Nets notes 3](https://cs231n.github.io/neural-networks-3/)。

翻译：神经网络笔记3[（上）](https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit)[（下）](https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit)。

> 该笔记讲解了神经网络的动态部分，即神经网络学习参数和搜索最优超参数的过程。内容列表如下：
>
> 梯度检查
>
> 合理性（Sanity）检查
>
> - 损失函数
> - 训练集与验证集准确率
> - 权重：更新比例
> - 每层的激活数据与梯度分布
> - 可视化 *译者注：上篇翻译截止处*
>
>
> - 一阶（随机梯度下降）方法，动量方法，Nesterov动量方法
> - 学习率退火
> - 二阶方法
> - 逐参数适应学习率方法（Adagrad，RMSProp）
>
> 超参数调优
>
> - 模型集成
>
> 总结
>
> 拓展引用





---

## 原文：[ConvNet notes](https://cs231n.github.io/convolutional-networks/)。

翻译：[卷积神经网络笔记](https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit)。

> 内容列表：
>
> - **结构概述**
> - **用来构建卷积神经网络的各种层**
>   卷积层汇聚层归一化层全连接层将全连接层转化成卷积层
> - **卷积神经网络的结构**层的排列规律层的尺寸设置规律案例学习（LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet）计算上的考量
> - **拓展资源**

- **完成了3个课程作业页面的翻译**：






---

## 原文：[[Assignment #1\]](https://cs231n.github.io/assignments2016/assignment1/)。

翻译：[CS231n课程作业#1简介](https://zhuanlan.zhihu.com/p/21441838?refer=intelligentunit)。

> 作业内容：实现k-NN，SVM分类器，Softmax分类器和两层神经网络，实践一个简单的图像分类流程。









---

## 原文：[[Assignment #2\]](https://cs231n.github.io/assignments2016/assignment2/)。

翻译：[CS231n课程作业#2简介](https://zhuanlan.zhihu.com/p/21941485?refer=intelligentunit)。

> 作业内容：练习编写反向传播代码，训练神经网络和卷积神经网络。





---

## 原文：[[Assignment #3\]](https://cs231n.github.io/assignments2016/assignment3/)。

翻译：[CS231n课程作业#3简介](https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit)。

> 作业内容：实现循环网络，并将其应用于在微软的COCO数据库上进行图像标注。实现DeepDream等有趣应用。

- **帮助知友@智靖远发起了在Youtube上合力翻译课程字幕的倡议**：

原文：[知友智靖远关于CS231n课程字幕翻译的倡议](https://zhuanlan.zhihu.com/p/21354230?refer=intelligentunit)。当时，[@智靖远](https://www.zhihu.com/people/313544833f1060900fcb4f6a75c9f6b6)已经贡献了他对第一课字幕的翻译，目前这个翻译项目仍在进行中，欢迎各位知友积极参与。具体操作方式在倡议原文中有，请大家点击查看。

有很多知友私信我们，询问为何不做字幕。现在统一答复：**请大家积极参加@智靖远的字幕翻译项目。**他先进行的字幕贡献与翻译，我们**不能夺人之美**。**后续，我们也会向该翻译项目进行贡献**。

## 翻译团队

CS231n课程笔记的翻译，始于[@杜客](https://www.zhihu.com/people/928affb05b0b70a2c12e109d63b6bae5)在一次回答问题“[应该选择TensorFlow还是Theano？](https://www.zhihu.com/question/41907061)”中的机缘巧合，在[取得了授权](https://zhuanlan.zhihu.com/p/20870307?refer=intelligentunit)后申请了知乎专栏[智能单元 - 知乎专栏](https://zhuanlan.zhihu.com/intelligentunit)独自翻译。随着翻译的进行，更多的知友参与进来。他们是[@ShiqingFan](https://www.zhihu.com/people/584f06e4ed2edc6007e4793179e7cdc1)，@[猴子](https://www.zhihu.com/people/hmonkey)，[@堃堃](https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9)和[@李艺颖](https://www.zhihu.com/people/f11e78650e8185db2b013af42fd9a481)。

**大家因为认同这件事而聚集在一起**，牺牲了很多个人的时间来进行翻译，校对和润色。而翻译的质量，我们不愿意自我表扬，还是**请各位知友自行阅读评价**吧。现在笔记翻译告一段落，下面是**团队成员的简短感言**：

[@ShiqingFan](https://www.zhihu.com/people/584f06e4ed2edc6007e4793179e7cdc1) ：一个偶然的机会让自己加入到这个翻译小队伍里来。CS231n给予了我知识的源泉和思考的灵感，前期的翻译工作也督促自己快速了学习了这门课程。虽然科研方向是大数据与并行计算，不过因为同时对深度学习比较感兴趣，于是乎现在的工作与两者都紧密相连。Merci!

@[猴子](https://www.zhihu.com/people/hmonkey)：在CS231n翻译小组工作的两个多月的时间非常难忘。我向杜客申请加入翻译小组的时候，才刚接触这门课不久，翻译和校对的工作让我对这门课的内容有了更深刻的理解。作为一个机器学习的初学者，我非常荣幸能和翻译小组一起工作并做一点贡献。希望以后能继续和翻译小组一起工作和学习。

[@堃堃](https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9) ：感谢组内各位成员的辛勤付出，很幸运能够参与这份十分有意义的工作，希望自己的微小工作能够帮助到大家，谢谢！

[@李艺颖](https://www.zhihu.com/people/f11e78650e8185db2b013af42fd9a481) ：当你真正沉下心来要做一件事情的时候才是学习和提高最好的状态；当你有热情做事时，并不会觉得是在牺牲时间，因为那是有意义并能带给你成就感和充实感的；不需要太过刻意地在乎大牛的巨大光芒，你只需像傻瓜一样坚持下去就好了，也许回头一看，你已前进了很多。就像老杜说的，我们就是每一步慢慢走，怎么就“零星”地把这件事给搞完了呢？

[@杜客](https://www.zhihu.com/people/928affb05b0b70a2c12e109d63b6bae5) ：做了一点微小的工作，哈哈。
